"""
BigQuery ëŒ€í™” ì„œë¹„ìŠ¤
ëŒ€í™” ì €ì¥/ì¡°íšŒ/ì‚­ì œ - ë¡œê·¸ì¸ í•„ìˆ˜ ë²„ì „
"""

import os
import json
import logging
from datetime import datetime, timezone
from typing import Dict, Any, List
from google.cloud import bigquery
from google.cloud.exceptions import NotFound

logger = logging.getLogger(__name__)

class ConversationService:
    """BigQuery ëŒ€í™” ê´€ë¦¬ ì„œë¹„ìŠ¤ - ë¡œê·¸ì¸ í•„ìˆ˜ ë²„ì „"""
    
    def __init__(self, project_id: str, location: str = "asia-northeast3"):
        """
        ConversationService ì´ˆê¸°í™”
        
        Args:
            project_id: Google Cloud í”„ë¡œì íŠ¸ ID
            location: BigQuery ë¦¬ì „
        """
        self.project_id = project_id
        self.location = location
        try:
            self.client = bigquery.Client(project=project_id, location=location)
            logger.info(f"BigQuery ConversationService ì´ˆê¸°í™” ì™„ë£Œ: {project_id}")
        except Exception as e:
            logger.error(f"BigQuery ConversationService ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            raise
    
    def save_conversation(self, conversation_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ëŒ€í™” ë‚´ìš©ì„ BigQueryì— ì €ì¥ (ë¡œê·¸ì¸ ì‚¬ìš©ì ì „ìš©)
        
        Args:
            conversation_data: ì €ì¥í•  ëŒ€í™” ë°ì´í„°
            
        Returns:
            ì €ì¥ ê²°ê³¼
        """
        try:
            # ëŒ€í™” í…Œì´ë¸” ì„¤ì • (í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •)
            dataset_name = os.getenv('CONVERSATION_DATASET', 'v1')
            table_id = f"{self.project_id}.{dataset_name}.conversations"
            
            # í•„ìˆ˜ í•„ë“œ ê²€ì¦
            required_fields = ['conversation_id', 'message_id', 'message', 'message_type', 'user_id']
            for field in required_fields:
                if field not in conversation_data:
                    raise ValueError(f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}")
            
            # ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ì •ë¦¬
            clean_data = self._clean_conversation_data(conversation_data)
            
            # BigQuery í…Œì´ë¸” ì°¸ì¡°
            table_ref = self.client.dataset(dataset_name).table('conversations')
            
            # ê³µí†µ í—¬í¼ë¡œ í…Œì´ë¸” ì¡´ì¬ í™•ì¸ ë° ìë™ ìƒì„±
            table_check_result = self._ensure_conversation_table_exists(dataset_name)
            if not table_check_result['success']:
                return {
                    "success": False,
                    "error": table_check_result['error']
                }
            
            # í…Œì´ë¸” ì°¸ì¡°
            table = self.client.get_table(table_ref)
            
            # ì„¸ì…˜ ë©”íƒ€ë°ì´í„° ê´€ë¦¬ (ì²« ë©”ì‹œì§€ì¸ ê²½ìš°)
            session_result = self._manage_session_metadata(conversation_data, dataset_name)
            if not session_result['success']:
                logger.warning(f"âš ï¸ ì„¸ì…˜ ë©”íƒ€ë°ì´í„° ê´€ë¦¬ ì‹¤íŒ¨: {session_result['error']}")
                # ì„¸ì…˜ ë©”íƒ€ë°ì´í„° ì‹¤íŒ¨í•´ë„ ëŒ€í™”ëŠ” ì €ì¥ ê³„ì†
            
            # ë©”ì‹œì§€ ë°ì´í„° ì‚½ì… (ìŠ¤íŠ¸ë¦¬ë° ì‚½ì… ì‚¬ìš©)
            logger.debug(f"ğŸ” BigQuery ì‚½ì… ë°ì´í„°: {clean_data}")
            errors = self.client.insert_rows_json(table, [clean_data])
            
            if errors:
                logger.error(f"ëŒ€í™” ì €ì¥ ì‹¤íŒ¨ - ìƒì„¸ ì˜¤ë¥˜: {errors}")
                # ì²« ë²ˆì§¸ ì˜¤ë¥˜ì˜ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
                first_error = errors[0] if errors else {}
                error_details = {
                    'index': first_error.get('index', 'N/A'),
                    'errors': first_error.get('errors', [])
                }
                logger.error(f"ëŒ€í™” ì €ì¥ ì˜¤ë¥˜ ìƒì„¸: {error_details}")
                return {
                    "success": False,
                    "error": f"ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error_details}"
                }
            
            logger.info(f"ğŸ’¾ ëŒ€í™” ì €ì¥ ì™„ë£Œ: {clean_data['conversation_id']} - {clean_data['message_type']} (ê²½ëŸ‰í™” ë²„ì „)")
            return {
                "success": True,
                "message": "ëŒ€í™”ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ (ê²½ëŸ‰í™” ë²„ì „)",
                "message_id": clean_data['message_id'],
                "optimized": True,
                "session_metadata": session_result.get('action', 'none')
            }
            
        except Exception as e:
            logger.error(f"ëŒ€í™” ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def get_user_conversations(self, user_id: str, limit: int = 50, offset: int = 0) -> Dict[str, Any]:
        """
        ì¸ì¦ëœ ì‚¬ìš©ìì˜ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ (í…Œì´ë¸” ìë™ ìƒì„± í¬í•¨)
        
        Args:
            user_id: ì‚¬ìš©ì ID
            limit: ìµœëŒ€ ì¡°íšŒ ê°œìˆ˜
            offset: ì˜¤í”„ì…‹
            
        Returns:
            ëŒ€í™” íˆìŠ¤í† ë¦¬ ëª©ë¡
        """
        try:
            dataset_name = os.getenv('CONVERSATION_DATASET', 'v1')
            conversations_table = f"{self.project_id}.{dataset_name}.conversations"
            
            # ê³µí†µ í—¬í¼ë¡œ í…Œì´ë¸” ì¡´ì¬ í™•ì¸ ë° ìë™ ìƒì„±
            table_check_result = self._ensure_conversation_table_exists(dataset_name)
            if not table_check_result['success']:
                # í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨ì‹œ ë¹ˆ ëª©ë¡ ë°˜í™˜
                return {
                    "success": True,
                    "conversations": [],
                    "count": 0,
                    "message": f"í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {table_check_result['error']}"
                }
            
            query = f"""
            SELECT 
                conversation_id,
                MIN(timestamp) as start_time,
                MAX(timestamp) as last_time,
                COUNT(*) as message_count,
                STRING_AGG(
                    CASE WHEN message_type = 'user' THEN message END, 
                    ' | ' 
                    ORDER BY timestamp 
                    LIMIT 1
                ) as first_message
            FROM `{conversations_table}`
            WHERE user_id = @user_id
            GROUP BY conversation_id
            ORDER BY start_time DESC
            LIMIT @limit OFFSET @offset
            """
            
            job_config = bigquery.QueryJobConfig(
                query_parameters=[
                    bigquery.ScalarQueryParameter("user_id", "STRING", user_id),
                    bigquery.ScalarQueryParameter("limit", "INT64", limit),
                    bigquery.ScalarQueryParameter("offset", "INT64", offset)
                ]
            )
            
            query_job = self.client.query(query, job_config=job_config)
            results = query_job.result()
            
            conversations = []
            for row in results:
                conversations.append({
                    "conversation_id": row.conversation_id,
                    "start_time": row.start_time.isoformat() if row.start_time else None,
                    "last_time": row.last_time.isoformat() if row.last_time else None,
                    "message_count": row.message_count,
                    "first_message": row.first_message or "ëŒ€í™” ì—†ìŒ"
                })
            
            return {
                "success": True,
                "conversations": conversations,
                "count": len(conversations)
            }
            
        except Exception as e:
            logger.error(f"ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "conversations": []
            }
    
    def get_conversation_details(self, conversation_id: str, user_id: str) -> Dict[str, Any]:
        """
        íŠ¹ì • ëŒ€í™”ì˜ ìƒì„¸ ë‚´ì—­ ì¡°íšŒ (ì‚¬ìš©ì ê¶Œí•œ í™•ì¸ í¬í•¨, í…Œì´ë¸” ìë™ ìƒì„± í¬í•¨)
        
        Args:
            conversation_id: ëŒ€í™” ID
            user_id: ì‚¬ìš©ì ID (ê¶Œí•œ í™•ì¸ìš©)
            
        Returns:
            ëŒ€í™” ìƒì„¸ ë‚´ì—­
        """
        try:
            dataset_name = os.getenv('CONVERSATION_DATASET', 'v1')
            conversations_table = f"{self.project_id}.{dataset_name}.conversations"
            
            # ê³µí†µ í—¬í¼ë¡œ í…Œì´ë¸” ì¡´ì¬ í™•ì¸ ë° ìë™ ìƒì„±
            table_check_result = self._ensure_conversation_table_exists(dataset_name)
            if not table_check_result['success']:
                # í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨ì‹œ ë¹ˆ ë©”ì‹œì§€ ëª©ë¡ ë°˜í™˜
                return {
                    "success": True,
                    "conversation_id": conversation_id,
                    "messages": [],
                    "message_count": 0
                }
            
            query = f"""
            SELECT 
                message_id,
                message,
                message_type,
                timestamp,
                query_type,
                generated_sql,
                execution_time_ms
            FROM `{conversations_table}`
            WHERE conversation_id = @conversation_id 
              AND user_id = @user_id
            ORDER BY timestamp ASC
            """
            
            job_config = bigquery.QueryJobConfig(
                query_parameters=[
                    bigquery.ScalarQueryParameter("conversation_id", "STRING", conversation_id),
                    bigquery.ScalarQueryParameter("user_id", "STRING", user_id)
                ]
            )
            
            query_job = self.client.query(query, job_config=job_config)
            results = query_job.result()
            
            messages = []
            for row in results:
                message_data = {
                    "message_id": row.message_id,
                    "message": row.message,
                    "message_type": row.message_type,
                    "timestamp": row.timestamp.isoformat() if row.timestamp else None,
                    "query_type": row.query_type,
                    "generated_sql": row.generated_sql,
                    "execution_time_ms": row.execution_time_ms
                }
                
                # assistant ë©”ì‹œì§€ì— ì¿¼ë¦¬ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°, ì €ì¥ëœ ê²°ê³¼ ì¡°íšŒ
                if row.message_type == 'assistant' and row.query_type == 'query_request' and row.generated_sql:
                    query_result = self.get_query_result(row.message_id, user_id)
                    if query_result['success']:
                        message_data['query_result_data'] = query_result['result_data']
                        message_data['query_row_count'] = query_result['row_count']
                
                messages.append(message_data)
            
            return {
                "success": True,
                "conversation_id": conversation_id,
                "messages": messages,
                "message_count": len(messages)
            }
            
        except Exception as e:
            logger.error(f"ëŒ€í™” ìƒì„¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "messages": []
            }
    
    def delete_conversation(self, conversation_id: str, user_id: str) -> Dict[str, Any]:
        """
        ì‚¬ìš©ìì˜ íŠ¹ì • ëŒ€í™” ì‚­ì œ (í…Œì´ë¸” ìë™ ìƒì„± í¬í•¨)
        
        Args:
            conversation_id: ì‚­ì œí•  ëŒ€í™” ID
            user_id: ì‚¬ìš©ì ID (ê¶Œí•œ í™•ì¸ìš©)
            
        Returns:
            ì‚­ì œ ê²°ê³¼
        """
        try:
            dataset_name = os.getenv('CONVERSATION_DATASET', 'v1')
            conversations_table = f"{self.project_id}.{dataset_name}.conversations"
            
            # ê³µí†µ í—¬í¼ë¡œ í…Œì´ë¸” ì¡´ì¬ í™•ì¸ ë° ìë™ ìƒì„±
            table_check_result = self._ensure_conversation_table_exists(dataset_name)
            if not table_check_result['success']:
                # í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ì‚­ì œí•  ëŒ€í™”ë„ ì—†ìŒ
                return {
                    "success": False,
                    "error": "ì‚­ì œí•  ëŒ€í™”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (í…Œì´ë¸” ì—†ìŒ)"
                }
            
            query = f"""
            DELETE FROM `{conversations_table}`
            WHERE conversation_id = @conversation_id 
              AND user_id = @user_id
            """
            
            job_config = bigquery.QueryJobConfig(
                query_parameters=[
                    bigquery.ScalarQueryParameter("conversation_id", "STRING", conversation_id),
                    bigquery.ScalarQueryParameter("user_id", "STRING", user_id)
                ]
            )
            
            query_job = self.client.query(query, job_config=job_config)
            query_job.result()  # ì™„ë£Œ ëŒ€ê¸°
            
            # ì‚­ì œëœ í–‰ ìˆ˜ í™•ì¸
            if query_job.num_dml_affected_rows > 0:
                logger.info(f"ëŒ€í™” ì‚­ì œ ì™„ë£Œ: {conversation_id} (ì‚¬ìš©ì: {user_id})")
                return {
                    "success": True,
                    "message": f"ëŒ€í™” {conversation_id}ê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤",
                    "deleted_rows": query_job.num_dml_affected_rows
                }
            else:
                return {
                    "success": False,
                    "error": "ì‚­ì œí•  ëŒ€í™”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                }
            
        except Exception as e:
            logger.error(f"ëŒ€í™” ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
        
    
    def _clean_conversation_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ëŒ€í™” ë°ì´í„°ë¥¼ BigQuery ì‚½ì…ì„ ìœ„í•´ ì •ë¦¬ (ê²½ëŸ‰í™” ë²„ì „)
        
        Args:
            data: ì›ë³¸ ëŒ€í™” ë°ì´í„°
            
        Returns:
            ê²½ëŸ‰í™”ëœ ì •ë¦¬ëœ ë°ì´í„°
        """
        # ê²½ëŸ‰í™”ëœ ê¸°ë³¸ í•„ë“œë§Œ í¬í•¨
        clean_data = {
            'conversation_id': data.get('conversation_id', ''),
            'message_id': data.get('message_id', ''),
            'user_id': data.get('user_id'),  # í•„ìˆ˜ í•„ë“œ
            'message': str(data.get('message', '')),
            'message_type': data.get('message_type', ''),
            'timestamp': data.get('timestamp'),
        }
        
        # ì¡°ê±´ë¶€ ì €ì¥ í•„ë“œë“¤
        query_type = data.get('query_type')
        if query_type:
            clean_data['query_type'] = query_type
        
        # SQLì€ query_request íƒ€ì…ì´ê³  ì‹¤ì œ SQLì´ ìˆëŠ” ê²½ìš°ë§Œ ì €ì¥
        generated_sql = data.get('generated_sql')
        if generated_sql and query_type == 'query_request':
            # SQL ê¸¸ì´ ì œí•œ (2KBë¡œ ì¶•ì†Œ)
            if len(generated_sql) > 2000:
                clean_data['generated_sql'] = generated_sql[:2000] + '...[truncated]'
            else:
                clean_data['generated_sql'] = generated_sql
        
        # execution_time_msëŠ” assistant ë©”ì‹œì§€ì´ê³  ê°’ì´ ìˆëŠ” ê²½ìš°ë§Œ (ì •ìˆ˜ë¡œ ë³€í™˜)
        execution_time_ms = data.get('execution_time_ms')
        if execution_time_ms is not None and data.get('message_type') == 'assistant':
            try:
                # ë¶€ë™ì†Œìˆ˜ì ì„ ì •ìˆ˜ë¡œ ë³€í™˜ (ë°€ë¦¬ì´ˆ ë‹¨ìœ„)
                clean_data['execution_time_ms'] = int(round(float(execution_time_ms)))
            except (ValueError, TypeError):
                # ë³€í™˜ ì‹¤íŒ¨ì‹œ Noneìœ¼ë¡œ ì„¤ì • (í•„ë“œ ìƒëµ)
                logger.warning(f"âš ï¸ execution_time_ms ë³€í™˜ ì‹¤íŒ¨: {execution_time_ms}")
                pass
        
        # timestamp ì²˜ë¦¬
        if not clean_data['timestamp']:
            clean_data['timestamp'] = datetime.now(timezone.utc).isoformat()
        
        # ë©”ì‹œì§€ ê¸¸ì´ ì œí•œ (3KBë¡œ ì¶•ì†Œ)
        if len(clean_data['message']) > 3000:
            clean_data['message'] = clean_data['message'][:3000] + '...[truncated]'
        
        return clean_data
    
    def _prepare_session_metadata(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ì„¸ì…˜ ë©”íƒ€ë°ì´í„° ì¤€ë¹„ (ì¤‘ë³µ ë°ì´í„° ë¶„ë¦¬)
        
        Args:
            data: ì›ë³¸ ëŒ€í™” ë°ì´í„°
            
        Returns:
            ì„¸ì…˜ ë©”íƒ€ë°ì´í„°
        """
        import hashlib
        
        # User-Agent í•´ì‹œí™” (ì €ì¥ ê³µê°„ ì ˆì•½)
        user_agent = data.get('user_agent', '')
        user_agent_hash = None
        if user_agent:
            user_agent_hash = hashlib.md5(user_agent.encode('utf-8')).hexdigest()
        
        session_data = {
            'conversation_id': data.get('conversation_id', ''),
            'user_id': data.get('user_id'),
            'user_email': data.get('user_email'),
            'ip_address': data.get('ip_address'),
            'user_agent_hash': user_agent_hash,
            'session_start': data.get('timestamp') or datetime.now(timezone.utc).isoformat(),
            'last_activity': data.get('timestamp') or datetime.now(timezone.utc).isoformat(),
            'message_count': 1
        }
        
        return session_data
    
    def _manage_session_metadata(self, data: Dict[str, Any], dataset_name: str) -> Dict[str, Any]:
        """
        ì„¸ì…˜ ë©”íƒ€ë°ì´í„° ê´€ë¦¬ (MERGEë¬¸ì„ ì‚¬ìš©í•œ UPSERT ë°©ì‹)
        
        Args:
            data: ëŒ€í™” ë°ì´í„°
            dataset_name: ë°ì´í„°ì…‹ ì´ë¦„
            
        Returns:
            ì„¸ì…˜ ë©”íƒ€ë°ì´í„° ì²˜ë¦¬ ê²°ê³¼
        """
        try:
            conversation_id = data.get('conversation_id')
            if not conversation_id:
                return {"success": False, "error": "conversation_idê°€ ì—†ìŠµë‹ˆë‹¤"}
            
            # session_metadata í…Œì´ë¸” ì¡´ì¬ í™•ì¸ ë° ìƒì„±
            try:
                metadata_table_ref = self.client.dataset(dataset_name).table('session_metadata')
                metadata_table = self.client.get_table(metadata_table_ref)
                logger.debug(f"ğŸ“Š session_metadata í…Œì´ë¸” ì¡´ì¬ í™•ì¸")
            except NotFound:
                logger.info(f"ğŸ”§ session_metadata í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤. ìë™ ìƒì„±ì„ ì‹œë„í•©ë‹ˆë‹¤.")
                metadata_creation_result = self._create_session_metadata_table(dataset_name)
                if not metadata_creation_result['success']:
                    logger.warning(f"âš ï¸ session_metadata í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨, conversationsë§Œ ì‚¬ìš©: {metadata_creation_result['error']}")
                    return {
                        "success": True,
                        "action": "skipped",
                        "reason": f"session_metadata ìƒì„± ì‹¤íŒ¨: {metadata_creation_result['error']}"
                    }
                metadata_table = self.client.get_table(metadata_table_ref)
            
            # ì„¸ì…˜ ë©”íƒ€ë°ì´í„° ì¤€ë¹„
            session_data = self._prepare_session_metadata(data)
            current_timestamp = datetime.now(timezone.utc)
            
            # MERGEë¬¸ì„ ì‚¬ìš©í•œ UPSERT (ìŠ¤íŠ¸ë¦¬ë° ë²„í¼ ë¬¸ì œ í•´ê²°)
            merge_query = f"""
            MERGE `{self.project_id}.{dataset_name}.session_metadata` T
            USING (
                SELECT 
                    @conversation_id as conversation_id,
                    @user_id as user_id,
                    @user_email as user_email,
                    @ip_address as ip_address,
                    @user_agent_hash as user_agent_hash,
                    @session_start as session_start,
                    @last_activity as last_activity
            ) S
            ON T.conversation_id = S.conversation_id
            WHEN MATCHED THEN
                UPDATE SET 
                    last_activity = S.last_activity,
                    message_count = T.message_count + 1
            WHEN NOT MATCHED THEN
                INSERT (
                    conversation_id, user_id, user_email, ip_address, 
                    user_agent_hash, session_start, last_activity, message_count
                )
                VALUES (
                    S.conversation_id, S.user_id, S.user_email, S.ip_address,
                    S.user_agent_hash, S.session_start, S.last_activity, 1
                )
            """
            
            job_config = bigquery.QueryJobConfig(
                query_parameters=[
                    bigquery.ScalarQueryParameter("conversation_id", "STRING", conversation_id),
                    bigquery.ScalarQueryParameter("user_id", "STRING", session_data['user_id']),
                    bigquery.ScalarQueryParameter("user_email", "STRING", session_data['user_email']),
                    bigquery.ScalarQueryParameter("ip_address", "STRING", session_data['ip_address']),
                    bigquery.ScalarQueryParameter("user_agent_hash", "STRING", session_data['user_agent_hash']),
                    bigquery.ScalarQueryParameter("session_start", "TIMESTAMP", 
                        datetime.fromisoformat(session_data['session_start'].replace('Z', '+00:00'))),
                    bigquery.ScalarQueryParameter("last_activity", "TIMESTAMP", current_timestamp)
                ]
            )
            
            merge_job = self.client.query(merge_query, job_config=job_config)
            merge_job.result()  # ì™„ë£Œ ëŒ€ê¸°
            
            return {
                "success": True,
                "action": "merged",
                "note": "MERGEë¬¸ì„ ì‚¬ìš©í•œ UPSERT ì™„ë£Œ"
            }
            
        except Exception as e:
            logger.error(f"âŒ ì„¸ì…˜ ë©”íƒ€ë°ì´í„° ê´€ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "success": True,  # ì„¸ì…˜ ë©”íƒ€ë°ì´í„° ì‹¤íŒ¨í•´ë„ ëŒ€í™” ì €ì¥ì€ ê³„ì†
                "action": "failed",
                "reason": f"ì„¸ì…˜ ë©”íƒ€ë°ì´í„° ì˜¤ë¥˜: {str(e)}"
            }
    
    # === ê³µí†µ í—¬í¼ ë©”ì„œë“œ ===
    
    def _ensure_conversation_table_exists(self, dataset_name: str) -> Dict[str, Any]:
        """
        conversations í…Œì´ë¸” ì¡´ì¬ í™•ì¸ ë° ìë™ ìƒì„± (ê³µí†µ í—¬í¼)
        
        Args:
            dataset_name: ë°ì´í„°ì…‹ ì´ë¦„
            
        Returns:
            í…Œì´ë¸” í™•ì¸/ìƒì„± ê²°ê³¼
        """
        conversations_table = f"{self.project_id}.{dataset_name}.conversations"
        table_ref = self.client.dataset(dataset_name).table('conversations')
        
        try:
            self.client.get_table(table_ref)
            logger.debug(f"ğŸ“‹ í…Œì´ë¸” {conversations_table} ì¡´ì¬ í™•ì¸")
            return {"success": True, "action": "exists"}
            
        except NotFound:
            logger.info(f"ğŸ”§ í…Œì´ë¸” {conversations_table}ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìë™ ìƒì„±ì„ ì‹œë„í•©ë‹ˆë‹¤.")
            table_creation_result = self._ensure_tables_exist(dataset_name)
            
            if table_creation_result['success']:
                logger.info(f"âœ… í…Œì´ë¸” {conversations_table} ìë™ ìƒì„± ì™„ë£Œ")
                return {"success": True, "action": "created"}
            else:
                logger.error(f"âŒ í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {table_creation_result['error']}")
                return {
                    "success": False, 
                    "error": f"í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {table_creation_result['error']}"
                }
        except Exception as e:
            logger.error(f"âŒ í…Œì´ë¸” í™•ì¸ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
            return {"success": False, "error": f"í…Œì´ë¸” í™•ì¸ ì˜¤ë¥˜: {str(e)}"}
    
    # === í…Œì´ë¸” ìë™ ìƒì„± ë° ê´€ë¦¬ ë©”ì„œë“œë“¤ ===
    
    def _ensure_tables_exist(self, dataset_name: str) -> Dict[str, Any]:
        """
        í•„ìš”í•œ í…Œì´ë¸”ë“¤ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ìƒì„±
        
        Args:
            dataset_name: BigQuery ë°ì´í„°ì…‹ ì´ë¦„
            
        Returns:
            í…Œì´ë¸” ìƒì„± ê²°ê³¼
        """
        try:
            # ë¨¼ì € ë°ì´í„°ì…‹ ì¡´ì¬ í™•ì¸ ë° ìƒì„±
            dataset_result = self._ensure_dataset_exists(dataset_name)
            if not dataset_result['success']:
                return dataset_result
            
            # conversations í…Œì´ë¸” ìƒì„±
            conversations_result = self._create_conversations_table(dataset_name)
            if not conversations_result['success']:
                return conversations_result
                
            # session_metadata í…Œì´ë¸” ìƒì„±  
            metadata_result = self._create_session_metadata_table(dataset_name)
            if not metadata_result['success']:
                return metadata_result
            
            # query_results í…Œì´ë¸” ìƒì„±
            query_results_result = self._create_query_results_table(dataset_name)
            if not query_results_result['success']:
                return query_results_result
            
            logger.info(f"âœ… ëª¨ë“  í•„ìˆ˜ í…Œì´ë¸” ìƒì„± ì™„ë£Œ: {dataset_name}")
            return {
                "success": True,
                "message": "ëª¨ë“  í…Œì´ë¸”ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤",
                "tables_created": ["conversations", "session_metadata", "query_results"]
            }
            
        except Exception as e:
            logger.error(f"âŒ í…Œì´ë¸” ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "success": False,
                "error": f"í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {str(e)}"
            }
    
    def _ensure_dataset_exists(self, dataset_name: str) -> Dict[str, Any]:
        """ë°ì´í„°ì…‹ ì¡´ì¬ í™•ì¸ ë° ìƒì„±"""
        try:
            dataset_ref = self.client.dataset(dataset_name)
            
            try:
                dataset = self.client.get_dataset(dataset_ref)
                logger.debug(f"ğŸ“‚ ë°ì´í„°ì…‹ {dataset_name} ì¡´ì¬ í™•ì¸")
                return {"success": True, "message": "ë°ì´í„°ì…‹ ì¡´ì¬í•¨"}
            except NotFound:
                # ë°ì´í„°ì…‹ ìƒì„±
                dataset = bigquery.Dataset(dataset_ref)
                dataset.location = self.location
                dataset.description = f"AAA ëŒ€í™” ì €ì¥ìš© ë°ì´í„°ì…‹ (ìë™ ìƒì„±: {datetime.now(timezone.utc).isoformat()})"
                
                dataset = self.client.create_dataset(dataset, timeout=30)
                logger.info(f"ğŸ“‚ ë°ì´í„°ì…‹ ìë™ ìƒì„±: {dataset_name}")
                
                return {
                    "success": True, 
                    "message": f"ë°ì´í„°ì…‹ {dataset_name} ìƒì„± ì™„ë£Œ"
                }
                
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„°ì…‹ ìƒì„±/í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "success": False,
                "error": f"ë°ì´í„°ì…‹ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
            }
    
    def _create_conversations_table(self, dataset_name: str) -> Dict[str, Any]:
        """ìµœì í™”ëœ conversations í…Œì´ë¸” ìƒì„±"""
        try:
            table_id = f"{self.project_id}.{dataset_name}.conversations"
            table_ref = self.client.dataset(dataset_name).table('conversations')
            
            # ìµœì í™”ëœ ìŠ¤í‚¤ë§ˆ ì •ì˜ (ê²½ëŸ‰í™” ì ìš©)
            schema = [
                bigquery.SchemaField("conversation_id", "STRING", mode="REQUIRED", description="ëŒ€í™” ì„¸ì…˜ ID"),
                bigquery.SchemaField("message_id", "STRING", mode="REQUIRED", description="ë©”ì‹œì§€ ê³ ìœ  ID"), 
                bigquery.SchemaField("user_id", "STRING", mode="REQUIRED", description="ì‚¬ìš©ì ID"),
                bigquery.SchemaField("message", "STRING", mode="REQUIRED", description="ë©”ì‹œì§€ ë‚´ìš© (ìµœëŒ€ 3KB)"),
                bigquery.SchemaField("message_type", "STRING", mode="REQUIRED", description="ë©”ì‹œì§€ íƒ€ì…: user, assistant"),
                bigquery.SchemaField("query_type", "STRING", mode="NULLABLE", description="ì¿¼ë¦¬ ë¶„ë¥˜: query_request, metadata_request ë“±"),
                bigquery.SchemaField("generated_sql", "STRING", mode="NULLABLE", description="ìƒì„±ëœ SQL (ìµœëŒ€ 2KB)"),
                bigquery.SchemaField("timestamp", "TIMESTAMP", mode="REQUIRED", description="ë©”ì‹œì§€ ìƒì„± ì‹œê°„"),
                bigquery.SchemaField("execution_time_ms", "INTEGER", mode="NULLABLE", description="ì‹¤í–‰ ì‹œê°„ (ë°€ë¦¬ì´ˆ)"),
            ]
            
            table = bigquery.Table(table_ref, schema=schema)
            
            # í…Œì´ë¸” ì„¤ì •
            table.description = "AAA ëŒ€í™” ë©”ì‹œì§€ ì €ì¥ (ê²½ëŸ‰í™” ë²„ì „)"
            table.time_partitioning = bigquery.TimePartitioning(
                type_=bigquery.TimePartitioningType.DAY,
                field="timestamp"
            )
            
            # í…Œì´ë¸” ìƒì„±
            table = self.client.create_table(table)
            logger.info(f"ğŸ“‹ conversations í…Œì´ë¸” ìƒì„± ì™„ë£Œ: {table_id}")
            
            return {
                "success": True,
                "message": f"conversations í…Œì´ë¸” ìƒì„± ì™„ë£Œ",
                "table_id": table_id
            }
            
        except Exception as e:
            logger.error(f"âŒ conversations í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return {
                "success": False,
                "error": f"conversations í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {str(e)}"
            }
    
    def _create_session_metadata_table(self, dataset_name: str) -> Dict[str, Any]:
        """ì„¸ì…˜ ë©”íƒ€ë°ì´í„° í…Œì´ë¸” ìƒì„± (ì¤‘ë³µ ë°ì´í„° ë¶„ë¦¬ìš©)"""
        try:
            table_id = f"{self.project_id}.{dataset_name}.session_metadata"
            table_ref = self.client.dataset(dataset_name).table('session_metadata')
            
            schema = [
                bigquery.SchemaField("conversation_id", "STRING", mode="REQUIRED", description="ëŒ€í™” ì„¸ì…˜ ID"),
                bigquery.SchemaField("user_id", "STRING", mode="REQUIRED", description="ì‚¬ìš©ì ID"),
                bigquery.SchemaField("user_email", "STRING", mode="REQUIRED", description="ì‚¬ìš©ì ì´ë©”ì¼"),
                bigquery.SchemaField("ip_address", "STRING", mode="NULLABLE", description="í´ë¼ì´ì–¸íŠ¸ IP ì£¼ì†Œ"),
                bigquery.SchemaField("user_agent_hash", "STRING", mode="NULLABLE", description="User-Agent í•´ì‹œê°’"),
                bigquery.SchemaField("session_start", "TIMESTAMP", mode="REQUIRED", description="ì„¸ì…˜ ì‹œì‘ ì‹œê°„"),
                bigquery.SchemaField("last_activity", "TIMESTAMP", mode="REQUIRED", description="ë§ˆì§€ë§‰ í™œë™ ì‹œê°„"),
                bigquery.SchemaField("message_count", "INTEGER", mode="NULLABLE", description="ì„¸ì…˜ ë‚´ ë©”ì‹œì§€ ìˆ˜"),
            ]
            
            table = bigquery.Table(table_ref, schema=schema)
            table.description = "AAA ì„¸ì…˜ë³„ ë©”íƒ€ë°ì´í„° (ì¤‘ë³µ ë°ì´í„° ë¶„ë¦¬)"
            
            # í…Œì´ë¸” ìƒì„±
            table = self.client.create_table(table)
            logger.info(f"ğŸ“Š session_metadata í…Œì´ë¸” ìƒì„± ì™„ë£Œ: {table_id}")
            
            return {
                "success": True,
                "message": f"session_metadata í…Œì´ë¸” ìƒì„± ì™„ë£Œ",
                "table_id": table_id
            }
            
        except Exception as e:
            logger.error(f"âŒ session_metadata í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return {
                "success": False,
                "error": f"session_metadata í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {str(e)}"
            }
    
    def _create_query_results_table(self, dataset_name: str) -> Dict[str, Any]:
        """ì¿¼ë¦¬ ê²°ê³¼ ì €ì¥ìš© ë³„ë„ í…Œì´ë¸” ìƒì„±"""
        try:
            table_id = f"{self.project_id}.{dataset_name}.query_results"
            table_ref = self.client.dataset(dataset_name).table('query_results')
            
            schema = [
                bigquery.SchemaField("result_id", "STRING", mode="REQUIRED", description="ì¿¼ë¦¬ ê²°ê³¼ ê³ ìœ  ID"),
                bigquery.SchemaField("message_id", "STRING", mode="REQUIRED", description="ì—°ê²°ëœ ë©”ì‹œì§€ ID"),
                bigquery.SchemaField("conversation_id", "STRING", mode="REQUIRED", description="ëŒ€í™” ì„¸ì…˜ ID"),
                bigquery.SchemaField("user_id", "STRING", mode="REQUIRED", description="ì‚¬ìš©ì ID"),
                bigquery.SchemaField("generated_sql", "STRING", mode="REQUIRED", description="ì‹¤í–‰ëœ SQL ì¿¼ë¦¬"),
                bigquery.SchemaField("result_data", "JSON", mode="REQUIRED", description="ì¿¼ë¦¬ ì‹¤í–‰ ê²°ê³¼ ë°ì´í„°"),
                bigquery.SchemaField("row_count", "INTEGER", mode="NULLABLE", description="ê²°ê³¼ í–‰ ìˆ˜"),
                bigquery.SchemaField("data_size_bytes", "INTEGER", mode="NULLABLE", description="ê²°ê³¼ ë°ì´í„° í¬ê¸°(ë°”ì´íŠ¸)"),
                bigquery.SchemaField("execution_time_ms", "INTEGER", mode="NULLABLE", description="ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„(ë°€ë¦¬ì´ˆ)"),
                bigquery.SchemaField("created_at", "TIMESTAMP", mode="REQUIRED", description="ê²°ê³¼ ì €ì¥ ì‹œê°„"),
                bigquery.SchemaField("expires_at", "TIMESTAMP", mode="NULLABLE", description="ë§Œë£Œ ì‹œê°„ (30ì¼ í›„)"),
            ]
            
            table = bigquery.Table(table_ref, schema=schema)
            table.description = "AAA ì¿¼ë¦¬ ì‹¤í–‰ ê²°ê³¼ ì €ì¥ (ëŒ€ìš©ëŸ‰ ê²°ê³¼ ì „ìš©)"
            
            # íŒŒí‹°ì…”ë‹ ì„¤ì • (created_at ê¸°ì¤€)
            table.time_partitioning = bigquery.TimePartitioning(
                type_=bigquery.TimePartitioningType.DAY,
                field="created_at"
            )
            
            # í…Œì´ë¸” ìƒì„±
            table = self.client.create_table(table)
            logger.info(f"ğŸ“Š query_results í…Œì´ë¸” ìƒì„± ì™„ë£Œ: {table_id}")
            
            return {
                "success": True,
                "message": f"query_results í…Œì´ë¸” ìƒì„± ì™„ë£Œ",
                "table_id": table_id
            }
            
        except Exception as e:
            logger.error(f"âŒ query_results í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return {
                "success": False,
                "error": f"query_results í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {str(e)}"
            }
    
    def save_query_result(self, query_result_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ì¿¼ë¦¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë³„ë„ í…Œì´ë¸”ì— ì €ì¥
        
        Args:
            query_result_data: ì¿¼ë¦¬ ê²°ê³¼ ë°ì´í„°
            {
                'message_id': str,
                'conversation_id': str, 
                'user_id': str,
                'generated_sql': str,
                'result_data': List[Dict],
                'row_count': int,
                'execution_time_ms': int
            }
            
        Returns:
            ì €ì¥ ê²°ê³¼
        """
        try:
            import json
            import uuid
            from datetime import datetime, timezone, timedelta
            
            dataset_name = os.getenv('CONVERSATION_DATASET', 'v1')
            
            # query_results í…Œì´ë¸” ì¡´ì¬ í™•ì¸ ë° ìƒì„±
            table_ref = self.client.dataset(dataset_name).table('query_results')
            try:
                table = self.client.get_table(table_ref)
                logger.debug(f"ğŸ“Š query_results í…Œì´ë¸” ì¡´ì¬ í™•ì¸")
            except NotFound:
                logger.info(f"ğŸ”§ query_results í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤. ìë™ ìƒì„±ì„ ì‹œë„í•©ë‹ˆë‹¤.")
                creation_result = self._create_query_results_table(dataset_name)
                if not creation_result['success']:
                    return {
                        "success": False,
                        "error": f"query_results í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {creation_result['error']}"
                    }
                table = self.client.get_table(table_ref)
            
            # ê²°ê³¼ ë°ì´í„° ìµœì í™”
            result_data = query_result_data.get('result_data', [])
            result_json = json.dumps(result_data, ensure_ascii=False)
            data_size_bytes = len(result_json.encode('utf-8'))
            
            # ì €ì¥í•  ë°ì´í„° ì¤€ë¹„
            current_time = datetime.now(timezone.utc)
            expires_time = current_time + timedelta(days=30)  # 30ì¼ í›„ ë§Œë£Œ
            
            # execution_time_msë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜
            execution_time_ms = query_result_data.get('execution_time_ms')
            if execution_time_ms is not None:
                try:
                    execution_time_ms = int(round(float(execution_time_ms)))
                except (ValueError, TypeError):
                    logger.warning(f"âš ï¸ query_results.execution_time_ms ë³€í™˜ ì‹¤íŒ¨: {execution_time_ms}, Noneìœ¼ë¡œ ì €ì¥")
                    execution_time_ms = None
            
            save_data = {
                'result_id': str(uuid.uuid4()),
                'message_id': query_result_data.get('message_id'),
                'conversation_id': query_result_data.get('conversation_id'),
                'user_id': query_result_data.get('user_id'),
                'generated_sql': query_result_data.get('generated_sql', ''),
                'result_data': result_json,  # JSON ë¬¸ìì—´ë¡œ ì €ì¥
                'row_count': query_result_data.get('row_count', 0),
                'data_size_bytes': data_size_bytes,
                'execution_time_ms': execution_time_ms,
                'created_at': current_time.isoformat(),
                'expires_at': expires_time.isoformat()
            }
            
            # í…Œì´ë¸”ì— ì‚½ì…
            errors = self.client.insert_rows_json(table, [save_data])
            
            if errors:
                logger.error(f"ì¿¼ë¦¬ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {errors}")
                return {
                    "success": False,
                    "error": f"ì¿¼ë¦¬ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {errors[0] if errors else 'Unknown error'}"
                }
            
            logger.info(f"ğŸ’¾ ì¿¼ë¦¬ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {save_data['result_id']} ({data_size_bytes:,} bytes)")
            return {
                "success": True,
                "result_id": save_data['result_id'],
                "data_size_bytes": data_size_bytes,
                "row_count": save_data['row_count']
            }
            
        except Exception as e:
            logger.error(f"âŒ ì¿¼ë¦¬ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "success": False,
                "error": f"ì¿¼ë¦¬ ê²°ê³¼ ì €ì¥ ì˜¤ë¥˜: {str(e)}"
            }
    
    def get_query_result(self, message_id: str, user_id: str) -> Dict[str, Any]:
        """
        ì €ì¥ëœ ì¿¼ë¦¬ ê²°ê³¼ ì¡°íšŒ
        
        Args:
            message_id: ë©”ì‹œì§€ ID
            user_id: ì‚¬ìš©ì ID (ê¶Œí•œ í™•ì¸ìš©)
            
        Returns:
            ì¿¼ë¦¬ ê²°ê³¼ ë°ì´í„°
        """
        try:
            dataset_name = os.getenv('CONVERSATION_DATASET', 'v1')
            table_name = f"{self.project_id}.{dataset_name}.query_results"
            
            query = f"""
            SELECT 
                result_id,
                generated_sql,
                result_data,
                row_count,
                data_size_bytes,
                execution_time_ms,
                created_at
            FROM `{table_name}`
            WHERE message_id = @message_id 
              AND user_id = @user_id
              AND (expires_at IS NULL OR expires_at > CURRENT_TIMESTAMP())
            ORDER BY created_at DESC
            LIMIT 1
            """
            
            job_config = bigquery.QueryJobConfig(
                query_parameters=[
                    bigquery.ScalarQueryParameter("message_id", "STRING", message_id),
                    bigquery.ScalarQueryParameter("user_id", "STRING", user_id)
                ]
            )
            
            query_job = self.client.query(query, job_config=job_config)
            results = list(query_job.result())
            
            if not results:
                return {
                    "success": False,
                    "error": "ì €ì¥ëœ ì¿¼ë¦¬ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                }
            
            row = results[0]
            result_data = row.result_data
            
            # JSON ë¬¸ìì—´ì„ íŒŒì´ì¬ ê°ì²´ë¡œ íŒŒì‹±
            if isinstance(result_data, str):
                try:
                    result_data = json.loads(result_data)
                except json.JSONDecodeError:
                    logger.warning(f"âš ï¸ result_data JSON íŒŒì‹± ì‹¤íŒ¨: message_id={message_id}")
            
            return {
                "success": True,
                "result_id": row.result_id,
                "generated_sql": row.generated_sql,
                "result_data": result_data,
                "row_count": row.row_count,
                "data_size_bytes": row.data_size_bytes,
                "execution_time_ms": row.execution_time_ms,
                "created_at": row.created_at.isoformat() if row.created_at else None
            }
            
        except Exception as e:
            logger.error(f"âŒ ì¿¼ë¦¬ ê²°ê³¼ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "success": False,
                "error": f"ì¿¼ë¦¬ ê²°ê³¼ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}"
            }
    
    def get_conversation_context(self, conversation_id: str, user_id: str, 
                               max_messages: int = 3) -> Dict[str, Any]:
        """
        LLM ì»¨í…ìŠ¤íŠ¸ìš© ëŒ€í™” ê¸°ë¡ ì¡°íšŒ (ìµœê·¼ Nê°œ ë©”ì‹œì§€)
        
        Args:
            conversation_id: ëŒ€í™” ì„¸ì…˜ ID
            user_id: ì‚¬ìš©ì ID (ê¶Œí•œ í™•ì¸)
            max_messages: ìµœëŒ€ ë©”ì‹œì§€ ìˆ˜ (ê¸°ë³¸ 3ê°œ)
        
        Returns:
            LLM í˜¸ì¶œìš© ëŒ€í™” ì»¨í…ìŠ¤íŠ¸
        """
        try:
            dataset_name = os.getenv('CONVERSATION_DATASET', 'v1')
            query = f"""
            SELECT 
                message,
                message_type,
                timestamp,
                query_type,
                generated_sql,
                message_id
            FROM `{self.project_id}.{dataset_name}.conversations`
            WHERE conversation_id = @conversation_id 
              AND user_id = @user_id
            ORDER BY timestamp DESC
            LIMIT @max_messages
            """
            
            job_config = bigquery.QueryJobConfig(
                query_parameters=[
                    bigquery.ScalarQueryParameter("conversation_id", "STRING", conversation_id),
                    bigquery.ScalarQueryParameter("user_id", "STRING", user_id),
                    bigquery.ScalarQueryParameter("max_messages", "INT64", max_messages)
                ]
            )
            
            query_job = self.client.query(query, job_config=job_config)
            results = list(query_job.result())
            
            # ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ì´ ë§ˆì§€ë§‰)
            messages = []
            for row in reversed(results):
                messages.append({
                    "role": "user" if row.message_type == "user" else "assistant",
                    "content": row.message,
                    "timestamp": row.timestamp.isoformat() if row.timestamp else None,
                    "metadata": {
                        "query_type": row.query_type,
                        "generated_sql": row.generated_sql
                    }
                })
            
            logger.info(f"ğŸ”„ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ ì™„ë£Œ: {conversation_id} ({len(messages)}ê°œ ë©”ì‹œì§€)")
            return {
                "success": True,
                "conversation_id": conversation_id,
                "messages": messages,
                "context_length": len(messages)
            }
            
        except Exception as e:
            logger.error(f"âŒ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "messages": []
            }
    
    def get_user_recent_context(self, user_id: str, max_messages: int = 5, 
                               exclude_conversation_id: str = None) -> Dict[str, Any]:
        """
        ì‚¬ìš©ì ê¸°ë°˜ ìµœê·¼ ëŒ€í™” ê¸°ë¡ ì¡°íšŒ (conversation_idì™€ ë¬´ê´€í•˜ê²Œ)
        
        Args:
            user_id: ì‚¬ìš©ì ID
            max_messages: ìµœëŒ€ ë©”ì‹œì§€ ìˆ˜ (ê¸°ë³¸ 5ê°œ)
            exclude_conversation_id: ì œì™¸í•  ëŒ€í™” ID (í˜„ì¬ ëŒ€í™” ì œì™¸ìš©)
        
        Returns:
            LLM í˜¸ì¶œìš© ëŒ€í™” ì»¨í…ìŠ¤íŠ¸
        """
        try:
            dataset_name = os.getenv('CONVERSATION_DATASET', 'v1')
            
            # í˜„ì¬ ëŒ€í™” ì œì™¸ ì¡°ê±´
            exclude_condition = ""
            query_params = [
                bigquery.ScalarQueryParameter("user_id", "STRING", user_id),
                bigquery.ScalarQueryParameter("max_messages", "INT64", max_messages)
            ]
            
            if exclude_conversation_id:
                exclude_condition = "AND conversation_id != @exclude_conversation_id"
                query_params.append(
                    bigquery.ScalarQueryParameter("exclude_conversation_id", "STRING", exclude_conversation_id)
                )
            
            query = f"""
            SELECT 
                message,
                message_type,
                timestamp,
                query_type,
                generated_sql,
                conversation_id,
                message_id
            FROM `{self.project_id}.{dataset_name}.conversations`
            WHERE user_id = @user_id
              {exclude_condition}
            ORDER BY timestamp DESC
            LIMIT @max_messages
            """
            
            job_config = bigquery.QueryJobConfig(query_parameters=query_params)
            query_job = self.client.query(query, job_config=job_config)
            results = list(query_job.result())
            
            # ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ì´ ë§ˆì§€ë§‰)
            messages = []
            for row in reversed(results):
                messages.append({
                    "role": "user" if row.message_type == "user" else "assistant",
                    "content": row.message,
                    "timestamp": row.timestamp.isoformat() if row.timestamp else None,
                    "metadata": {
                        "query_type": row.query_type,
                        "generated_sql": row.generated_sql,
                        "conversation_id": row.conversation_id,
                        "message_id": row.message_id
                    }
                })
            
            logger.info(f"ğŸ”„ ì‚¬ìš©ì ìµœê·¼ ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ ì™„ë£Œ: {user_id} ({len(messages)}ê°œ ë©”ì‹œì§€)")
            return {
                "success": True,
                "user_id": user_id,
                "messages": messages,
                "context_length": len(messages)
            }
            
        except Exception as e:
            logger.error(f"âŒ ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "messages": []
            }
    
    def optimize_context_size(self, messages: List[Dict], max_tokens: int = 2000) -> List[Dict]:
        """
        í† í° ì œí•œì— ë§ì¶° ì»¨í…ìŠ¤íŠ¸ ìµœì í™”
        
        Args:
            messages: ì›ë³¸ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
            max_tokens: ìµœëŒ€ í† í° ìˆ˜ (í•œê¸€ ê¸°ì¤€ ìµœì í™”: ê¸°ë³¸ 2000í† í°)
        
        Returns:
            ìµœì í™”ëœ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # í•œê¸€ í† í° ì¶”ì • ìµœì í™” (1í† í° â‰ˆ 2-3ê¸€ì, ì•ˆì „í•˜ê²Œ 2.5 ì ìš©)
            total_chars = sum(len(msg['content']) for msg in messages)
            estimated_tokens = total_chars / 2.5  # í•œê¸€ íŠ¹ì„± ê³ ë ¤
            
            if estimated_tokens <= max_tokens:
                logger.debug(f"ğŸ“Š ì»¨í…ìŠ¤íŠ¸ í¬ê¸° ì ì ˆ: {estimated_tokens:.0f} í† í° (ì œí•œ: {max_tokens})")
                return messages
            
            # ìµœì‹  ë©”ì‹œì§€ë¶€í„° ìœ ì§€í•˜ë©´ì„œ í¬ê¸° ì¡°ì ˆ
            optimized_messages = []
            current_chars = 0
            
            for message in reversed(messages):
                msg_chars = len(message['content'])
                if (current_chars + msg_chars) / 2.5 > max_tokens:
                    break
                optimized_messages.insert(0, message)
                current_chars += msg_chars
            
            optimized_tokens = current_chars / 2.5
            logger.info(f"ğŸ“Š ì»¨í…ìŠ¤íŠ¸ ìµœì í™” ì™„ë£Œ: {len(messages)} â†’ {len(optimized_messages)}ê°œ ë©”ì‹œì§€ ({estimated_tokens:.0f} â†’ {optimized_tokens:.0f} í† í°)")
            
            return optimized_messages
            
        except Exception as e:
            logger.error(f"âŒ ì»¨í…ìŠ¤íŠ¸ ìµœì í™” ì˜¤ë¥˜: {str(e)}")
            return messages[:3]  # ì‹¤íŒ¨ì‹œ ìµœê·¼ 3ê°œë§Œ ë°˜í™˜
    
    def get_table_schemas(self) -> Dict[str, Any]:
        """
        í˜„ì¬ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¡°íšŒ (ë””ë²„ê¹…/ëª¨ë‹ˆí„°ë§ìš©)
        
        Returns:
            í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ë³´
        """
        try:
            dataset_name = os.getenv('CONVERSATION_DATASET', 'v1')
            
            result = {
                "dataset": dataset_name,
                "tables": {}
            }
            
            # conversations í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ
            try:
                conversations_table = self.client.get_table(f"{self.project_id}.{dataset_name}.conversations")
                result["tables"]["conversations"] = {
                    "exists": True,
                    "num_rows": conversations_table.num_rows,
                    "size_bytes": conversations_table.num_bytes,
                    "created": conversations_table.created.isoformat() if conversations_table.created else None,
                    "schema": [{"name": field.name, "type": field.field_type, "mode": field.mode} 
                             for field in conversations_table.schema]
                }
            except NotFound:
                result["tables"]["conversations"] = {"exists": False}
            
            # session_metadata í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ
            try:
                metadata_table = self.client.get_table(f"{self.project_id}.{dataset_name}.session_metadata")
                result["tables"]["session_metadata"] = {
                    "exists": True,
                    "num_rows": metadata_table.num_rows,
                    "size_bytes": metadata_table.num_bytes,
                    "created": metadata_table.created.isoformat() if metadata_table.created else None,
                    "schema": [{"name": field.name, "type": field.field_type, "mode": field.mode} 
                             for field in metadata_table.schema]
                }
            except NotFound:
                result["tables"]["session_metadata"] = {"exists": False}
            
            return {
                "success": True,
                "schemas": result
            }
            
        except Exception as e:
            logger.error(f"âŒ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }