"""
í†µí•© LLM í´ë¼ì´ì–¸íŠ¸ - í”„ë¡¬í”„íŠ¸ ì¤‘ì•™ ê´€ë¦¬ ì‹œìŠ¤í…œ ì ìš©
ë¦¬íŒ©í† ë§: í•˜ë“œì½”ë”©ëœ í”„ë¡¬í”„íŠ¸ë¥¼ JSON íŒŒì¼ ê¸°ë°˜ ì‹œìŠ¤í…œìœ¼ë¡œ êµì²´
"""

import json
import logging
import re
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any
import anthropic

# í”„ë¡¬í”„íŠ¸ ì¤‘ì•™ ê´€ë¦¬ ì‹œìŠ¤í…œ ì„í¬íŠ¸
from .prompts import prompt_manager
# MetaSync ìºì‹œ ë¡œë” ì„í¬íŠ¸
from .metasync_cache_loader import get_metasync_cache_loader
# ContextBlock ì„í¬íŠ¸
from models import ContextBlock, context_blocks_to_llm_format

logger = logging.getLogger(__name__)


class BaseLLMClient(ABC):
    """LLM í´ë¼ì´ì–¸íŠ¸ ì¶”ìƒ ë² ì´ìŠ¤ í´ë˜ìŠ¤"""
    
    @abstractmethod
    def classify_input(self, user_input: str, context_blocks: List[ContextBlock] = None) -> dict:
        """ì‚¬ìš©ì ì…ë ¥ ë¶„ë¥˜"""
        pass
    
    @abstractmethod
    def generate_sql(self, question: str, project_id: str, dataset_ids: list = None, 
                   context_blocks: List[ContextBlock] = None) -> dict:
        """SQL ìƒì„±"""
        pass
    
    @abstractmethod
    def analyze_data(self, question: str, 
                   context_blocks: List[ContextBlock] = None) -> dict:
        """ë°ì´í„° ë¶„ì„"""
        pass
    
    @abstractmethod
    def generate_guide(self, question: str, context: str = "") -> dict:
        """ê°€ì´ë“œ ìƒì„±"""
        pass
        
    @abstractmethod
    def generate_out_of_scope(self, question: str) -> dict:
        """ë²”ìœ„ ì™¸ ì‘ë‹µ ìƒì„±"""
        pass


class AnthropicLLMClient(BaseLLMClient):
    """Anthropic Claude LLM í´ë¼ì´ì–¸íŠ¸ - í”„ë¡¬í”„íŠ¸ ì¤‘ì•™ ê´€ë¦¬ ë²„ì „"""
    
    def __init__(self, api_key: str):
        """
        Anthropic í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        
        Args:
            api_key: Anthropic API í‚¤
        """
        self.api_key = api_key
        try:
            self.client = anthropic.Anthropic(api_key=api_key)
            # MetaSync ìºì‹œ ë¡œë” ì´ˆê¸°í™”
            self.cache_loader = get_metasync_cache_loader()
            logger.info("âœ… Anthropic LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ (í”„ë¡¬í”„íŠ¸ ì¤‘ì•™ ê´€ë¦¬ + MetaSync)")
        except Exception as e:
            logger.error(f"âŒ Anthropic í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            raise
    
    def classify_input(self, user_input: str, context_blocks: List[ContextBlock] = None) -> dict:
        """
        ì‚¬ìš©ì ì…ë ¥ì„ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜ (ContextBlock ì‚¬ìš©)
        
        Args:
            user_input: ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
            context_blocks: ì´ì „ ëŒ€í™” ContextBlock ë¦¬ìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
            
        Returns:
            ë¶„ë¥˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # ContextBlockì„ LLM í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        llm_context = context_blocks_to_llm_format(context_blocks) if context_blocks else []
        
        return self._execute_unified_prompting(
            category='classification',
            input_data={'user_input': user_input},
            context_blocks=context_blocks
        )
    
    
    def _normalize_conversation_context(self, conversation_context: List[Dict] = None) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ì •ê·œí™” - í•­ìƒ ë¬¸ìì—´ ë°˜í™˜"""
        if not conversation_context or len(conversation_context) == 0:
            return "[ì´ì „ ëŒ€í™” ì—†ìŒ]"
        
        return self._format_conversation_context(conversation_context)
    
    def _calculate_dynamic_tokens(self, category: str, context: str) -> int:
        """
        ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ì™€ ì¹´í…Œê³ ë¦¬ì— ë”°ë¥¸ ì§€ëŠ¥í˜• í† í° í• ë‹¹
        """
        # ê¸°ë³¸ í† í° ì„¤ì •
        base_tokens = {
            'classification': 300,     # ë¶„ë¥˜ëŠ” ì§§ì€ JSON ì‘ë‹µ
            'sql_generation': 1200,    # SQLì€ ë³µì¡í•œ ì¿¼ë¦¬ ê°€ëŠ¥
            'data_analysis': 1200,     # ë¶„ì„ì€ ìƒì„¸í•œ ì„¤ëª… í•„ìš”
            'guide_request': 800,      # ê°€ì´ë“œëŠ” ì¤‘ê°„ ê¸¸ì´
            'metadata_request': 600    # ë©”íƒ€ë°ì´í„°ëŠ” êµ¬ì¡°í™”ëœ ì‘ë‹µ
        }
        
        base = base_tokens.get(category, 400)
        
        # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë”°ë¥¸ ì¶”ê°€ í† í°
        if context == "[ì´ì „ ëŒ€í™” ì—†ìŒ]":
            return base
        
        # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì¸¡ì • (ëŒ€ëµì )
        context_length = len(context.split())
        
        if context_length < 50:
            context_bonus = 50      # ì§§ì€ ì»¨í…ìŠ¤íŠ¸
        elif context_length < 200:
            context_bonus = 100     # ì¤‘ê°„ ì»¨í…ìŠ¤íŠ¸  
        else:
            context_bonus = 200     # ê¸´ ì»¨í…ìŠ¤íŠ¸
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì»¨í…ìŠ¤íŠ¸ ë¯¼ê°ë„
        context_multiplier = {
            'classification': 0.5,    # ë¶„ë¥˜ëŠ” ì»¨í…ìŠ¤íŠ¸ ì˜í–¥ ì ìŒ
            'sql_generation': 1.0,    # SQLì€ ì»¨í…ìŠ¤íŠ¸ ì¤‘ìš”
            'data_analysis': 1.5,     # ë¶„ì„ì€ ì»¨í…ìŠ¤íŠ¸ ë§¤ìš° ì¤‘ìš”
            'guide_request': 0.7,
            'metadata_request': 0.3
        }
        
        multiplier = context_multiplier.get(category, 1.0)
        final_bonus = int(context_bonus * multiplier)
        
        return min(base + final_bonus, 2000)  # ìµœëŒ€ 2000 í† í° ì œí•œ

    def _execute_unified_prompting(self, 
                                 category: str,
                                 input_data: Dict[str, Any],
                                 context_blocks: List[ContextBlock] = None) -> dict:
        """
        í†µí•© í”„ë¡¬í”„íŒ… ì‹¤í–‰ - í•­ìƒ ê°™ì€ í…œí”Œë¦¿ ì‚¬ìš©
        
        Args:
            category: í”„ë¡¬í”„íŠ¸ ì¹´í…Œê³ ë¦¬
            input_data: ì…ë ¥ ë°ì´í„°
            context_blocks: ì´ì „ ëŒ€í™” ContextBlock ë¦¬ìŠ¤íŠ¸
            
        Returns:
            LLM ì‘ë‹µ ê²°ê³¼
        """
        try:
            # ContextBlockì„ LLM í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            llm_context = context_blocks_to_llm_format(context_blocks) if context_blocks else []
            
            # ë°ì´í„° ë¶„ì„: ë¹„ë³€í˜• ì›ì¹™ ì ìš© â€” ìš”ì•½/ì •ê·œí™” ëŒ€ì‹  ì›ë³¸ JSON ì „ë‹¬
            if category == 'data_analysis':
                import os
                import json as _json
                # ìµœê·¼ ê²°ê³¼ RAW í–‰ ì¶”ì¶œ ë° í¬ê¸° ì œí•œ ì ìš©
                raw_rows_all = self._extract_latest_result_rows(context_blocks or [])
                max_rows = int(os.getenv('ANALYSIS_MAX_ROWS', '200'))
                max_chars = int(os.getenv('ANALYSIS_MAX_CHARS', '60000'))
                raw_rows_json = self._pack_rows_as_json(raw_rows_all, max_rows=max_rows, max_chars=max_chars)
                try:
                    raw_rows = _json.loads(raw_rows_json)
                except Exception:
                    raw_rows = []

                # ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘(ì „ì²´ í–‰ ìˆ˜/ì¶œì²˜ ë¸”ë¡)
                source_block_id = None
                total_row_count = None
                for blk in reversed(context_blocks or []):
                    if getattr(blk, 'execution_result', None):
                        source_block_id = getattr(blk, 'block_id', None)
                        total_row_count = blk.execution_result.get('row_count')
                        break

                # ë‹¨ì¼ ì»¨í…ìŠ¤íŠ¸ JSON(envelope) êµ¬ì„± â€” data í‚¤ëŠ” RAW í–‰ì„ ê·¸ëŒ€ë¡œ í¬í•¨
                envelope = {
                    'messages': llm_context,
                    'data': raw_rows,
                    'meta': {
                        'row_count': total_row_count if total_row_count is not None else (len(raw_rows_all) if isinstance(raw_rows_all, list) else None),
                        'included_rows': len(raw_rows) if isinstance(raw_rows, list) else 0,
                        'source_block_id': source_block_id
                    },
                    'limits': {
                        'max_rows': max_rows,
                        'max_chars': max_chars
                    }
                }

                context_json = _json.dumps(envelope, ensure_ascii=False, separators=(',', ':'))
                normalized_context = "[context-json]"
            else:
                normalized_context = self._normalize_conversation_context(llm_context)
            
            # ì»¨í…ìŠ¤íŠ¸ í™•ì¸ (ì˜¤ë¥˜ ë°œìƒì‹œì—ë§Œ ë¡œê·¸)
            context_count = len(context_blocks) if context_blocks else 0
            
            # MetaSync ìºì‹œ ë°ì´í„° í†µí•© (SQL ìƒì„± ì‹œì—ë§Œ)
            enhanced_input_data = self._enhance_input_data_with_metasync(category, input_data)
            
            # ë‹¨ì¼ í…œí”Œë¦¿ ì‚¬ìš© (í–¥ìƒëœ ë°ì´í„° ì‚¬ìš©)
            system_prompt = prompt_manager.get_prompt(
                category=category,
                template_name='system_prompt',
                **enhanced_input_data,
                fallback_prompt=self._get_fallback_system_prompt(category)
            )
            
            if category == 'data_analysis':
                user_prompt = prompt_manager.get_prompt(
                    category=category,
                    template_name='user_prompt',
                    context_json=context_json,
                    **enhanced_input_data,
                    fallback_prompt=self._get_fallback_user_prompt(category, enhanced_input_data)
                )
                # ë°ì´í„° ë¶„ì„ì€ ì¶©ë¶„í•œ í† í°ì„ ê³ ì • í• ë‹¹(ìƒí•œ 2000)
                max_tokens = 2000
            else:
                user_prompt = prompt_manager.get_prompt(
                    category=category,
                    template_name='user_prompt',
                    context_blocks=normalized_context,
                    **enhanced_input_data,
                    fallback_prompt=self._get_fallback_user_prompt(category, enhanced_input_data)
                )
                # ë™ì  í† í° í• ë‹¹
                max_tokens = self._calculate_dynamic_tokens(category, normalized_context)
            
            # Claude API í˜¸ì¶œ
            response = self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=max_tokens,
                system=system_prompt,
                messages=[{"role": "user", "content": user_prompt}]
            )
            
            response_text = response.content[0].text.strip()
            return self._post_process_response(category, response_text, normalized_context)
            
        except (anthropic.RateLimitError, anthropic.APIStatusError) as e:
            logger.warning(f"âš ï¸ Anthropic API ì†ë„ ì œí•œ ë˜ëŠ” ê³¼ë¶€í•˜: {str(e)}")
            return {
                "success": False,
                "error": "AI ì„œë¹„ìŠ¤ê°€ ì¼ì‹œì ìœ¼ë¡œ ìš”ì²­ì´ ë§ì•„ ì‘ë‹µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "error_type": "rate_limit_error"
            }
        except Exception as e:
            logger.error(f"âŒ í†µí•© í”„ë¡¬í”„íŒ… ì˜¤ë¥˜ ({category}): {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }

    def _extract_latest_result_rows(self, blocks: List[ContextBlock]) -> List[Dict[str, Any]]:
        """ê°€ì¥ ìµœê·¼ ContextBlockì—ì„œ RAW ê²°ê³¼ í–‰ì„ ì¶”ì¶œ(ë¹„ë³€í˜•)"""
        try:
            for blk in reversed(blocks):
                if getattr(blk, 'execution_result', None):
                    data = blk.execution_result.get('data')
                    if isinstance(data, list) and data:
                        return data
            return []
        except Exception:
            return []

    def _pack_rows_as_json(self, rows: List[Dict[str, Any]], max_rows: int = 200, max_chars: int = 60000) -> str:
        """RAW í–‰ ë¦¬ìŠ¤íŠ¸ë¥¼ JSON ë¬¸ìì—´ë¡œ ì§ë ¬í™”(ë¬´ì†ì‹¤, í¬ê¸° ì œí•œë§Œ ì ìš©)"""
        import json as _json
        if not rows:
            return "[]"
        n = min(len(rows), max_rows)
        while n >= 1:
            chunk = rows[:n]
            s = _json.dumps(chunk, ensure_ascii=False, separators=(',', ':'))
            if len(s) <= max_chars:
                return s
            # í¬ê¸° ì´ˆê³¼ ì‹œ í–‰ ìˆ˜ë¥¼ ì¤„ì—¬ ì¬ì‹œë„(70% ë¹„ìœ¨ë¡œ ê°ì†Œ)
            new_n = int(n * 0.7)
            n = new_n if new_n < n else n - 1
        # ìµœì†Œ 1í–‰ë„ ì´ˆê³¼í•˜ë©´ ë¹ˆ ë°°ì—´ ë°˜í™˜
        return "[]"

    def _format_analysis_context(self, context_messages: List[Dict[str, Any]]) -> str:
        """ë°ì´í„° ë¶„ì„ìš©ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ë¥¼ í’ë¶€í•˜ê²Œ ìš”ì•½ (ê²°ê³¼ ìƒ˜í”Œ í¬í•¨)

        - ìµœê·¼ ë©”ì‹œì§€ 5ê°œ ë‚´ì—ì„œ, assistantì˜ ì¿¼ë¦¬ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì»¬ëŸ¼ê³¼ ìƒìœ„ 2í–‰ì„ í¬í•¨
        - ê¸¸ì´ ì œí•œì„ ìœ„í•´ ê° í–‰ì€ ìµœëŒ€ 3ê°œ ì»¬ëŸ¼ë§Œ í‘œì‹œ
        """
        if not context_messages:
            return "[ì´ì „ ëŒ€í™” ì—†ìŒ]"

        lines: List[str] = []
        # ìµœê·¼ 5ê°œ ë©”ì‹œì§€ë§Œ ê³ ë ¤
        msgs = context_messages[-5:]
        for msg in msgs:
            role = "ì‚¬ìš©ì" if msg.get('role') == 'user' else 'AI'
            timestamp = msg.get('timestamp', '')[:19] if msg.get('timestamp') else ''
            content = msg.get('content', '') or ''
            if len(content) > 200:
                content = content[:200] + '...'

            # ê¸°ë³¸ ë¼ì¸
            base = f"[{timestamp}] {role}: {content}"

            # ë¶„ì„ì— ìœ ìš©í•œ ë¶€ê°€ ì •ë³´ (assistant ìª½ ê²°ê³¼ ìš”ì•½)
            sample_added = False
            if role == 'AI':
                row_count = msg.get('query_row_count') or 0
                if row_count:
                    base += f" (ê²°ê³¼ {row_count}í–‰)"
                result_rows = msg.get('query_result_data')
                if isinstance(result_rows, list) and result_rows:
                    # ì»¬ëŸ¼ ì¶”ì¶œ ë° ìƒìœ„ 2í–‰ ìƒ˜í”Œ
                    first_row = result_rows[0]
                    if isinstance(first_row, dict):
                        columns = list(first_row.keys())[:3]
                        lines.append(base)
                        lines.append(f"  â–· ì»¬ëŸ¼: {', '.join(columns)}")
                        # ìµœëŒ€ 2í–‰ ìƒ˜í”Œ
                        for i, row in enumerate(result_rows[:2]):
                            row_vals = [str(row.get(c)) for c in columns]
                            lines.append(f"  â–· ìƒ˜í”Œí–‰{i+1}: {', '.join(row_vals)}")
                        sample_added = True
            if not sample_added:
                lines.append(base)

        return "\n".join(lines) if lines else "[ì´ì „ ëŒ€í™” ì—†ìŒ]"

    def _format_conversation_context(self, context: List[Dict]) -> str:
        """ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ LLM í”„ë¡¬í”„íŠ¸ìš© í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        if not context:
            return ""
        
        formatted_lines = []
        for msg in context[-3:]:  # ìµœê·¼ 5ê°œ ë©”ì‹œì§€ë§Œ ì‚¬ìš©
            role = "ì‚¬ìš©ì" if msg['role'] == "user" else "AI"
            timestamp = msg.get('timestamp', '')[:19] if msg.get('timestamp') else ''
            content = msg['content'][:200] + "..." if len(msg['content']) > 200 else msg['content']
            
            # SQL ì •ë³´ê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€
            sql_info = ""
            if msg.get('metadata', {}).get('generated_sql'):
                sql_info = f" [SQL ìƒì„±í•¨]"
            
            formatted_lines.append(f"[{timestamp}] {role}: {content}{sql_info}")
        
        return "\n".join(formatted_lines)
    
    # ì¹´í…Œê³ ë¦¬ë³„ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ê¸°ë“¤
    def _process_classification_context(self, context: List[Dict]) -> Dict[str, Any]:
        """ë¶„ë¥˜ìš© ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬"""
        return {'conversation_context': self._format_conversation_context(context)}
    
    def _process_sql_context(self, context: List[Dict]) -> Dict[str, Any]:
        """SQL ìƒì„±ìš© ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬"""
        return {
            'conversation_context': self._format_conversation_context(context),
            'previous_sqls': self._extract_sql_patterns(context),
            'frequently_used_tables': self._extract_table_usage(context)
        }
    
    def _process_analysis_context(self, context: List[Dict]) -> Dict[str, Any]:
        """ë¶„ì„ìš© ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬"""
        return {
            'conversation_context': self._format_conversation_context(context),
            'previous_analysis': self._extract_previous_analysis(context)
        }
    
    def _extract_sql_patterns(self, context: List[Dict]) -> str:
        """ì´ì „ ëŒ€í™”ì—ì„œ SQL íŒ¨í„´ ì¶”ì¶œ"""
        sql_patterns = []
        for msg in context:
            if msg.get('metadata', {}).get('generated_sql'):
                sql = msg['metadata']['generated_sql']
                if sql and len(sql) > 20:
                    sql_patterns.append(sql[:100] + "...")
        
        return "\n".join(sql_patterns) if sql_patterns else "ì´ì „ SQL ì—†ìŒ"
    
    def _extract_table_usage(self, context: List[Dict]) -> str:
        """ìì£¼ ì‚¬ìš©ë˜ëŠ” í…Œì´ë¸” íŒ¨í„´ ì¶”ì¶œ"""
        tables = []
        for msg in context:
            if msg.get('metadata', {}).get('generated_sql'):
                sql = msg['metadata']['generated_sql']
                if sql and 'FROM' in sql.upper():
                    # ê°„ë‹¨í•œ í…Œì´ë¸”ëª… ì¶”ì¶œ
                    import re
                    table_matches = re.findall(r'FROM\s+`([^`]+)`', sql, re.IGNORECASE)
                    tables.extend(table_matches)
        
        unique_tables = list(set(tables))
        return ", ".join(unique_tables) if unique_tables else "ê¸°ë³¸ í…Œì´ë¸” ì‚¬ìš©"
    
    def _extract_previous_analysis(self, context: List[Dict]) -> str:
        """ì´ì „ ë¶„ì„ ê²°ê³¼ ì¶”ì¶œ"""
        analyses = []
        for msg in context:
            if msg['role'] == 'assistant' and 'ë¶„ì„' in msg['content']:
                analyses.append(msg['content'][:150] + "...")
        
        return "\n".join(analyses) if analyses else "ì´ì „ ë¶„ì„ ì—†ìŒ"
    
    def _post_process_response(self, category: str, response_text: str, normalized_context: str) -> dict:
        """ì¹´í…Œê³ ë¦¬ë³„ ì‘ë‹µ í›„ì²˜ë¦¬"""
        if category == 'classification':
            try:
                classification = json.loads(response_text)
                if all(key in classification for key in ["category", "confidence"]):
                    context_info = f" (ì»¨í…ìŠ¤íŠ¸: ìˆìŒ)" if normalized_context != "[ì´ì „ ëŒ€í™” ì—†ìŒ]" else " (ì»¨í…ìŠ¤íŠ¸: ì—†ìŒ)"
                    logger.info(f"ğŸ¯ í†µí•© ë¶„ë¥˜: {classification['category']}{context_info}")
                    return {"success": True, "classification": classification}
                else:
                    raise ValueError("í•„ìˆ˜ í•„ë“œ ëˆ„ë½")
            except (json.JSONDecodeError, ValueError):
                return {
                    "success": True,
                    "classification": {
                        "category": "query_request",
                        "confidence": 0.5,
                        "reasoning": "í†µí•© ë¶„ë¥˜ íŒŒì‹± ì‹¤íŒ¨ë¡œ ê¸°ë³¸ê°’ ì‚¬ìš©"
                    }
                }
        elif category == 'sql_generation':
            cleaned_sql = self._clean_sql_response(response_text)
            logger.info(f"ğŸ”§ í†µí•© SQL ìƒì„± ì™„ë£Œ: {cleaned_sql[:100]}...")
            return {
                "success": True,
                "sql": cleaned_sql,
                "raw_response": response_text
            }
        elif category == 'data_analysis':
            logger.info(f"ğŸ” í†µí•© ë°ì´í„° ë¶„ì„ ì™„ë£Œ")
            return {
                "success": True,
                "analysis": response_text
            }
        else:
            return {
                "success": True,
                "response": response_text
            }
    
    def _get_fallback_system_prompt(self, category: str) -> str:
        """ì¹´í…Œê³ ë¦¬ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ Fallback"""
        fallbacks = {
            'classification': self._get_fallback_classification_prompt(),
            'sql_generation': "BigQuery SQL ì „ë¬¸ê°€ë¡œì„œ ìì—°ì–´ë¥¼ SQLë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.",
            'data_analysis': "ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ë¡œì„œ ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”."
        }
        return fallbacks.get(category, "ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ë¡œì„œ ë„ì›€ì„ ì œê³µí•´ì£¼ì„¸ìš”.")
    
    def _get_fallback_user_prompt(self, category: str, input_data: Dict[str, Any]) -> str:
        """ì¹´í…Œê³ ë¦¬ë³„ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ Fallback"""
        if category == 'classification':
            return f"ë¶„ë¥˜í•  ì…ë ¥: {input_data.get('user_input', '')}"
        elif category == 'sql_generation':
            return f"SQL ìƒì„± ì§ˆë¬¸: {input_data.get('question', '')}"
        elif category == 'data_analysis':
            return f"ë¶„ì„ ì§ˆë¬¸: {input_data.get('question', '')}"
        else:
            return str(input_data)
    
    def generate_sql(self, question: str, project_id: str, dataset_ids: list = None, 
                   context_blocks: List[ContextBlock] = None) -> dict:
        """
        ìì—°ì–´ ì§ˆë¬¸ì„ BigQuery SQLë¡œ ë³€í™˜ (ContextBlock ì‚¬ìš©)
        
        Args:
            question: ì‚¬ìš©ìì˜ ìì—°ì–´ ì§ˆë¬¸
            project_id: BigQuery í”„ë¡œì íŠ¸ ID  
            dataset_ids: ì‚¬ìš©í•  ë°ì´í„°ì…‹ ID ëª©ë¡ (ì„ íƒì‚¬í•­)
            context_blocks: ì´ì „ ëŒ€í™” ContextBlock ë¦¬ìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
            
        Returns:
            SQL ìƒì„± ê²°ê³¼
        """
        default_table = "`nlq-ex.test_dataset.events_20210131`"
        
        # ContextBlockì„ LLM í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        llm_context = context_blocks_to_llm_format(context_blocks) if context_blocks else []
        
        return self._execute_unified_prompting(
            category='sql_generation',
            input_data={
                'question': question,
                'project_id': project_id,
                'default_table': default_table
            },
            context_blocks=context_blocks
        )

    def analyze_data(self, question: str, context_blocks: List[ContextBlock] = None) -> dict:
        """
        ì»¨í…ìŠ¤í‹° ê¸°ë°˜ ë¶„ì„ ìƒì„±
        
        Args:
            question: ì‚¬ìš©ìì˜ ë¶„ì„ ìš”ì²­ ì§ˆë¬¸
            context_blocks: ì´ì „ ëŒ€í™” ContextBlock ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ë°ì´í„° ë¶„ì„ ê²°ê³¼
        """
        
        # ë¹„ë³€í˜• ì›ì¹™: ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš© (ìë¥´ê¸°/ì²­í¬ëŠ” ì´í›„ ë‹¨ê³„ì—ì„œ ì²˜ë¦¬)
        recent_blocks = context_blocks or []
        
        # ë¸”ë¡ ì •ë³´ ë¡œê¹…
        if recent_blocks:
            logger.info(f"ğŸ§© ë¶„ì„ìš© ì»¨í…ìŠ¤íŠ¸: {len(recent_blocks)}ê°œ ë¸”ë¡")
        
        # ContextBlockì„ LLM í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì›ë³¸ í˜•íƒœ ìœ ì§€)
        llm_context = context_blocks_to_llm_format(recent_blocks) if recent_blocks else []

        return self._execute_unified_prompting(
            category='data_analysis',
            input_data={
                'question': question
            },
            context_blocks=recent_blocks
        )

    def generate_guide(self, question: str, context: str = "") -> dict:
        """
        ê°€ì´ë“œ ì‘ë‹µ ìƒì„± (í”„ë¡¬í”„íŠ¸ ì¤‘ì•™ ê´€ë¦¬ ì ìš©)
        
        Args:
            question: ì‚¬ìš©ìì˜ ê°€ì´ë“œ ìš”ì²­
            context: í˜„ì¬ ìƒí™© ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            ê°€ì´ë“œ ì‘ë‹µ ê²°ê³¼
        """
        try:
            # í”„ë¡¬í”„íŠ¸ ì¤‘ì•™ ê´€ë¦¬ ì‹œìŠ¤í…œì—ì„œ ë¡œë“œ
            guide_prompt = prompt_manager.get_prompt(
                category='guides',
                template_name='usage_guide',
                context=context,
                user_question=question,
                fallback_prompt=self._get_fallback_guide_prompt(question, context)
            )

            response = self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=1000,
                messages=[{"role": "user", "content": guide_prompt}]
            )
            
            guide = response.content[0].text.strip()
            logger.info(f"ğŸ’¡ ê°€ì´ë“œ ì‘ë‹µ ìƒì„± ì™„ë£Œ (ì¤‘ì•™ ê´€ë¦¬)")
            
            return {
                "success": True,
                "guide": guide
            }
            
        except Exception as e:
            logger.error(f"âŒ ê°€ì´ë“œ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "guide": None
            }
    
    def generate_out_of_scope(self, question: str) -> dict:
        """
        ê¸°ëŠ¥ ë²”ìœ„ ì™¸ ìš”ì²­ì— ëŒ€í•œ ì‘ë‹µ ìƒì„± (í”„ë¡¬í”„íŠ¸ ì¤‘ì•™ ê´€ë¦¬ ì ìš©)
        
        Args:
            question: ì‚¬ìš©ìì˜ ì§ˆë¬¸
            
        Returns:
            ë²”ìœ„ ì™¸ ì‘ë‹µ ê²°ê³¼
        """
        try:
            # í”„ë¡¬í”„íŠ¸ ì¤‘ì•™ ê´€ë¦¬ ì‹œìŠ¤í…œì—ì„œ ë¡œë“œ
            scope_prompt = prompt_manager.get_prompt(
                category='guides',
                template_name='out_of_scope',
                user_question=question,
                fallback_prompt=self._get_fallback_out_of_scope_prompt(question)
            )

            response = self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=400,
                messages=[{"role": "user", "content": scope_prompt}]
            )
            
            scope_response = response.content[0].text.strip()
            logger.info(f"ğŸš« ë²”ìœ„ ì™¸ ì‘ë‹µ ìƒì„± ì™„ë£Œ (ì¤‘ì•™ ê´€ë¦¬)")
            
            return {
                "success": True,
                "response": scope_response
            }
            
        except Exception as e:
            logger.error(f"âŒ ë²”ìœ„ ì™¸ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "response": None
            }
    
    def _clean_sql_response(self, raw_response: str) -> str:
        """Claude ì‘ë‹µì—ì„œ SQL ì¿¼ë¦¬ë§Œ ì¶”ì¶œí•˜ê³  ì •ë¦¬ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)"""
        
        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
        if '```sql' in raw_response:
            sql_match = re.search(r'```sql\s*\n(.*?)\n```', raw_response, re.DOTALL)
            if sql_match:
                raw_response = sql_match.group(1)
        elif '```' in raw_response:
            code_match = re.search(r'```(.*?)```', raw_response, re.DOTALL)
            if code_match:
                raw_response = code_match.group(1)
        
        # ì •ë¦¬ ê³¼ì •
        sql_query = raw_response.strip()
        
        # ì£¼ì„ ì œê±° (SQL í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì§€ ì•Šì€ ë¼ì¸ë§Œ)
        lines = sql_query.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            # ì£¼ì„ì´ì§€ë§Œ SQL êµ¬ë¬¸ì´ í¬í•¨ëœ ê²½ìš°ëŠ” ìœ ì§€
            if line.startswith('--') and not any(
                keyword in line.upper() 
                for keyword in ['SELECT', 'FROM', 'WHERE', 'GROUP', 'ORDER', 'LIMIT']
            ):
                continue
            cleaned_lines.append(line)
        
        # ì¬ì¡°í•©
        sql_query = '\n'.join(cleaned_lines)
        
        # ì„¸ë¯¸ì½œë¡  ì¶”ê°€
        if not sql_query.rstrip().endswith(';'):
            sql_query = sql_query.rstrip() + ';'
        
        return sql_query

    # === ì¶”ê°€ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ (í”„ë¡¬í”„íŠ¸ ì¤‘ì•™ ê´€ë¦¬ ì ìš©) ===
    
    def explain_query(self, sql_query: str, question: str) -> dict:
        """
        ìƒì„±ëœ SQL ì¿¼ë¦¬ì— ëŒ€í•œ ì„¤ëª… ìƒì„± (í”„ë¡¬í”„íŠ¸ ì¤‘ì•™ ê´€ë¦¬ ì ìš©)
        """
        try:
            explanation_prompt = prompt_manager.get_prompt(
                category='improvements',
                template_name='explain_query',
                user_question=question,
                sql_query=sql_query,
                fallback_prompt=self._get_fallback_explain_prompt(sql_query, question)
            )

            response = self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=600,
                messages=[{"role": "user", "content": explanation_prompt}]
            )
            
            explanation = response.content[0].text.strip()
            logger.info(f"ğŸ“ ì¿¼ë¦¬ ì„¤ëª… ìƒì„± ì™„ë£Œ (ì¤‘ì•™ ê´€ë¦¬)")
            
            return {
                "success": True,
                "explanation": explanation
            }
            
        except Exception as e:
            logger.error(f"âŒ ì¿¼ë¦¬ ì„¤ëª… ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "explanation": None
            }
    
    def suggest_improvements(self, sql_query: str) -> dict:
        """
        SQL ì¿¼ë¦¬ ê°œì„  ì‚¬í•­ ì œì•ˆ (í”„ë¡¬í”„íŠ¸ ì¤‘ì•™ ê´€ë¦¬ ì ìš©)
        """
        try:
            improvement_prompt = prompt_manager.get_prompt(
                category='improvements',
                template_name='suggest_improvements',
                sql_query=sql_query,
                fallback_prompt=self._get_fallback_improvement_prompt(sql_query)
            )

            response = self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=1500,
                messages=[{"role": "user", "content": improvement_prompt}]
            )
            
            suggestions = response.content[0].text.strip()
            logger.info(f"ğŸ’¡ ì¿¼ë¦¬ ê°œì„  ì œì•ˆ ìƒì„± ì™„ë£Œ (ì¤‘ì•™ ê´€ë¦¬)")
            
            return {
                "success": True,
                "suggestions": suggestions
            }
            
        except Exception as e:
            logger.error(f"âŒ ê°œì„  ì œì•ˆ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "suggestions": None
            }
    
    def generate_sample_questions(self, project_id: str, dataset_ids: list = None) -> dict:
        """
        í”„ë¡œì íŠ¸ì™€ ë°ì´í„°ì…‹ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒ˜í”Œ ì§ˆë¬¸ë“¤ì„ ìƒì„± (í”„ë¡¬í”„íŠ¸ ì¤‘ì•™ ê´€ë¦¬ ì ìš©)
        """
        try:
            dataset_info = ""
            if dataset_ids:
                dataset_list = ", ".join(dataset_ids)
                dataset_info = f"ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹: {dataset_list}"
            
            sample_prompt = prompt_manager.get_prompt(
                category='guides',
                template_name='sample_questions',
                project_id=project_id,
                dataset_info=dataset_info,
                fallback_prompt=self._get_fallback_sample_questions_prompt(project_id, dataset_info)
            )

            response = self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=1000,
                messages=[{"role": "user", "content": sample_prompt}]
            )
            
            response_text = response.content[0].text.strip()
            
            # JSON ë°°ì—´ íŒŒì‹± ì‹œë„
            try:
                questions = json.loads(response_text)
                if isinstance(questions, list):
                    logger.info(f"ğŸ“ ìƒ˜í”Œ ì§ˆë¬¸ {len(questions)}ê°œ ìƒì„± ì™„ë£Œ (ì¤‘ì•™ ê´€ë¦¬)")
                    return {
                        "success": True,
                        "questions": questions
                    }
            except json.JSONDecodeError:
                # íŒŒì‹± ì‹¤íŒ¨ì‹œ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
                questions = self._extract_questions_from_text(response_text)
                return {
                    "success": True,
                    "questions": questions
                }
            
            return {
                "success": False,
                "error": "ì‘ë‹µ í˜•ì‹ì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                "questions": []
            }
            
        except Exception as e:
            logger.error(f"âŒ ìƒ˜í”Œ ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "questions": []
            }
    
    def _extract_questions_from_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì§ˆë¬¸ë“¤ì„ ì¶”ì¶œí•˜ëŠ” í—¬í¼ í•¨ìˆ˜ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)"""
        questions = []
        
        # ë‹¤ì–‘í•œ íŒ¨í„´ìœ¼ë¡œ ì§ˆë¬¸ ì¶”ì¶œ
        patterns = [
            r'"([^"]+\?)"',  # ë”°ì˜´í‘œë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ì§ˆë¬¸
            r'(\d+\.\s+[^?\n]+\?)',  # ë²ˆí˜¸ê°€ ë¶™ì€ ì§ˆë¬¸
            r'([A-Zê°€-í£][^?\n]*\?)',  # ëŒ€ë¬¸ì/í•œê¸€ë¡œ ì‹œì‘í•˜ëŠ” ì§ˆë¬¸
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                clean_question = match.strip()
                if clean_question and len(clean_question) > 10:
                    questions.append(clean_question)
        
        # ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ 10ê°œë¡œ ì œí•œ
        unique_questions = list(dict.fromkeys(questions))[:10]
        
        # ê¸°ë³¸ ì§ˆë¬¸ë“¤ì´ ì—†ì„ ê²½ìš° ê¸°ë³¸ê°’ ì œê³µ
        if not unique_questions:
            unique_questions = [
                "ì „ì²´ ë°ì´í„°ì˜ í–‰ ìˆ˜ëŠ” ì–¼ë§ˆë‚˜ ë˜ë‚˜ìš”?",
                "ê°€ì¥ ìµœê·¼ ë°ì´í„°ëŠ” ì–¸ì œì¸ê°€ìš”?",
                "ê° ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„° ìˆ˜ëŠ”?",
                "ì›”ë³„ ë°ì´í„° ì¶”ì´ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
                "ìƒìœ„ 10ê°œ í•­ëª©ì€ ë¬´ì—‡ì¸ê°€ìš”?"
            ]
        
        return unique_questions

    # === MetaSync ìºì‹œ í†µí•© ë©”ì„œë“œë“¤ ===
    
    def _enhance_input_data_with_metasync(self, category: str, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ì¹´í…Œê³ ë¦¬ì— ë”°ë¼ MetaSync ìºì‹œ ë°ì´í„°ë¥¼ input_dataì— í†µí•©
        
        Args:
            category: í”„ë¡¬í”„íŠ¸ ì¹´í…Œê³ ë¦¬
            input_data: ê¸°ì¡´ ì…ë ¥ ë°ì´í„°
            
        Returns:
            MetaSync ë°ì´í„°ê°€ í†µí•©ëœ ì…ë ¥ ë°ì´í„°
        """
        enhanced_data = input_data.copy()
        
        # SQL ìƒì„± ê´€ë ¨ ì¹´í…Œê³ ë¦¬ì—ì„œë§Œ MetaSync ë°ì´í„° í™œìš©
        if category in ['query_request', 'sql_generation']:
            try:
                cached_data = self._get_cached_data_with_fallback()
                
                if cached_data['source'] == 'metasync':
                    pass  # MetaSync ë°ì´í„° ì ìš©
                    
                    # ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ê°€
                    enhanced_data['schema_columns'] = self._format_schema_for_prompt(
                        cached_data['schema_info'].get('columns', [])
                    )
                    
                    # Few-Shot ì˜ˆì‹œ ì¶”ê°€
                    enhanced_data['few_shot_examples'] = self._format_examples_for_prompt(
                        cached_data['examples']
                    )
                    
                    # í…Œì´ë¸” ID ì¶”ê°€
                    enhanced_data['table_id'] = cached_data['schema_info'].get(
                        'table_id', 'nlq-ex.test_dataset.events_20210131'
                    )
                    
                else:
                    logger.warning("âš ï¸ MetaSync ìºì‹œ ì‚¬ìš© ë¶ˆê°€, í´ë°± ë°ì´í„° ì‚¬ìš©")
                    enhanced_data.update(self._get_fallback_metasync_data())
                    
            except Exception as e:
                logger.error(f"âŒ MetaSync ë°ì´í„° í†µí•© ì‹¤íŒ¨: {e}")
                enhanced_data.update(self._get_fallback_metasync_data())
        
        return enhanced_data
    
    def _get_cached_data_with_fallback(self) -> Dict[str, Any]:
        """ìºì‹œ ë°ì´í„° ë¡œë“œ ë° í´ë°± ì²˜ë¦¬"""
        try:
            # MetaSync ìºì‹œ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
            if not self.cache_loader.is_cache_available():
                logger.warning("MetaSync cache not available, using fallback")
                return self._get_fallback_data()
            
            schema_info = self.cache_loader.get_schema_info()
            examples = self.cache_loader.get_few_shot_examples()
            
            # ë°ì´í„° ê²€ì¦
            if not schema_info.get('columns') or not examples:
                logger.warning("MetaSync cache data incomplete, using fallback")
                return self._get_fallback_data()
            
            return {
                'schema_info': schema_info,
                'examples': examples,
                'source': 'metasync'
            }
            
        except Exception as e:
            logger.error(f"Failed to load MetaSync cache: {e}")
            return self._get_fallback_data()
    
    def _get_fallback_data(self) -> Dict[str, Any]:
        """ìºì‹œ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë°ì´í„° ë°˜í™˜"""
        return {
            'schema_info': {
                'table_id': 'nlq-ex.test_dataset.events_20210131',
                'columns': []  # ë¹ˆ ìŠ¤í‚¤ë§ˆë¡œ ì²˜ë¦¬
            },
            'examples': [],  # ë¹ˆ ì˜ˆì‹œë¡œ ì²˜ë¦¬
            'source': 'fallback'
        }
    
    def _get_fallback_metasync_data(self) -> Dict[str, Any]:
        """MetaSync í´ë°± ë°ì´í„°"""
        return {
            'schema_columns': "ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í…Œì´ë¸” êµ¬ì¡°ë¥¼ ê°€ì •í•©ë‹ˆë‹¤.",
            'few_shot_examples': "ì˜ˆì‹œë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì¿¼ë¦¬ íŒ¨í„´ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.",
            'table_id': 'nlq-ex.test_dataset.events_20210131'
        }
    
    def _format_schema_for_prompt(self, columns: List[Dict[str, str]]) -> str:
        """ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ì— ì í•©í•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        if not columns:
            return "ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        formatted_columns = []
        for col in columns:
            col_desc = f"- {col['name']} ({col['type']})"
            if col.get('description'):
                col_desc += f": {col['description']}"
            formatted_columns.append(col_desc)
        
        return "\n".join(formatted_columns)
    
    def _format_examples_for_prompt(self, examples: List[Dict[str, str]]) -> str:
        """Few-Shot ì˜ˆì‹œë¥¼ í”„ë¡¬í”„íŠ¸ì— ì í•©í•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        if not examples:
            return "ì˜ˆì‹œë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        formatted_examples = []
        for i, example in enumerate(examples, 1):
            formatted_examples.append(f"""
                                        ì˜ˆì‹œ {i}:
                                        ì§ˆë¬¸: {example['question']}
                                        SQL: {example['sql']}
                                        """)
        
        return "\n".join(formatted_examples)
    
    def check_metasync_status(self) -> Dict[str, Any]:
        """MetaSync ìºì‹œ ìƒíƒœ í™•ì¸ (ë””ë²„ê¹…/ëª¨ë‹ˆí„°ë§ìš©)"""
        try:
            metadata = self.cache_loader.get_cache_metadata()
            return {
                'status': 'available' if self.cache_loader.is_cache_available() else 'unavailable',
                'generated_at': metadata.get('generated_at'),
                'examples_count': metadata.get('examples_count', 0),
                'columns_count': metadata.get('columns_count', 0),
                'table_id': metadata.get('table_id')
            }
        except Exception as e:
            return {'status': 'error', 'error': str(e)}

    # === Fallback í”„ë¡¬í”„íŠ¸ë“¤ (í”„ë¡¬í”„íŠ¸ ë¡œë”© ì‹¤íŒ¨ ì‹œ ì‚¬ìš©) ===
    
    def _get_fallback_classification_prompt(self) -> str:
        """ë¶„ë¥˜ í”„ë¡¬í”„íŠ¸ Fallback"""
        return """ì‚¬ìš©ì ì…ë ¥ì„ ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•˜ê³  JSONìœ¼ë¡œ ì‘ë‹µ:
                    1. query_request - ë°ì´í„° ì¡°íšŒ ìš”ì²­
                    2. data_analysis - ë°ì´í„° ë¶„ì„ ìš”ì²­
                    3. guide_request - ì‚¬ìš©ë²• ìš”ì²­
                    4. out_of_scope - ê¸°ëŠ¥ ë²”ìœ„ ì™¸

                    JSON í˜•ì‹: {"category": "ë¶„ë¥˜", "confidence": 0.95}"""
    
    def _get_fallback_sql_system_prompt(self, project_id: str, default_table: str) -> str:
        """SQL ìƒì„± ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ Fallback"""
        return f"""BigQuery SQL ì „ë¬¸ê°€ë¡œì„œ ìì—°ì–´ë¥¼ SQLë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.
            í”„ë¡œì íŠ¸: {project_id}
            ê¸°ë³¸ í…Œì´ë¸”: {default_table}
            - SQLë§Œ ë°˜í™˜, ì„¸ë¯¸ì½œë¡  í•„ìˆ˜
            - LIMIT 100 ê¸°ë³¸ ì ìš©
            - TIMESTAMP_MICROS(event_timestamp) ì‚¬ìš©"""
    
    
    def _get_fallback_analysis_prompt(self, question: str, data_context: str) -> str:
        """ë°ì´í„° ë¶„ì„ í”„ë¡¬í”„íŠ¸ Fallback"""
        return f"""ë‹¤ìŒ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:
                {data_context}
                ì§ˆë¬¸: {question}
                ì£¼ìš” íŠ¹ì§•ê³¼ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”."""
    
    def _get_fallback_guide_prompt(self, question: str, context: str) -> str:
        """ê°€ì´ë“œ í”„ë¡¬í”„íŠ¸ Fallback"""
        return f"""BigQuery Assistant ì‚¬ìš©ë²•ì„ ì•ˆë‚´í•´ì£¼ì„¸ìš”.
                ìƒí™©: {context}
                ì§ˆë¬¸: {question}
                ì£¼ìš” ê¸°ëŠ¥ê³¼ ì‚¬ìš© ì˜ˆì‹œë¥¼ ì œê³µí•´ì£¼ì„¸ìš”."""
    
    def _get_fallback_out_of_scope_prompt(self, question: str) -> str:
        """ë²”ìœ„ ì™¸ ì‘ë‹µ Fallback"""
        return f"""ì£„ì†¡í•©ë‹ˆë‹¤. '{question}' ì§ˆë¬¸ì€ BigQuery Assistantì˜ ê¸°ëŠ¥ ë²”ìœ„ë¥¼ ë²—ì–´ë‚©ë‹ˆë‹¤.
                ëŒ€ì‹  ë°ì´í„° ì¡°íšŒ, ë¶„ì„, í…Œì´ë¸” ì •ë³´ ìš”ì²­ ë“±ì„ ë„ì™€ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."""
    
    def _get_fallback_explain_prompt(self, sql_query: str, question: str) -> str:
        """SQL ì„¤ëª… í”„ë¡¬í”„íŠ¸ Fallback"""
        return f"""ë‹¤ìŒ SQLì„ ì„¤ëª…í•´ì£¼ì„¸ìš”:
                ì›ë³¸ ì§ˆë¬¸: {question}
                SQL: {sql_query}
                ì¿¼ë¦¬ì˜ ëª©ì ê³¼ ê²°ê³¼ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”."""
    
    def _get_fallback_improvement_prompt(self, sql_query: str) -> str:
        """SQL ê°œì„  í”„ë¡¬í”„íŠ¸ Fallback"""
        return f"""ë‹¤ìŒ SQLì˜ ê°œì„  ë°©ì•ˆì„ ì œì•ˆí•´ì£¼ì„¸ìš”:
                {sql_query}
                ì„±ëŠ¥, ë¹„ìš©, ê°€ë…ì„± ê´€ì ì—ì„œ ê°œì„ ì•ˆì„ ì œì‹œí•´ì£¼ì„¸ìš”."""
                    
    def _get_fallback_sample_questions_prompt(self, project_id: str, dataset_info: str) -> str:
        """ìƒ˜í”Œ ì§ˆë¬¸ í”„ë¡¬í”„íŠ¸ Fallback"""
        return f"""í”„ë¡œì íŠ¸ {project_id}ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ìœ ìš©í•œ ì§ˆë¬¸ ì˜ˆì‹œë¥¼ JSON ë°°ì—´ë¡œ ì œê³µí•´ì£¼ì„¸ìš”.
                {dataset_info}
                ê¸°ë³¸ ì¡°íšŒ, ì§‘ê³„, ë¶„ì„ ë“± ë‹¤ì–‘í•œ ì§ˆë¬¸ì„ í¬í•¨í•´ì£¼ì„¸ìš”."""


class LLMClientFactory:
    """LLM í´ë¼ì´ì–¸íŠ¸ íŒ©í† ë¦¬ - í™•ì¥ ê°€ëŠ¥í•œ êµ¬ì¡° (í”„ë¡¬í”„íŠ¸ ì¤‘ì•™ ê´€ë¦¬ ì§€ì›)"""
    
    @staticmethod
    def create_client(provider: str, config: dict) -> BaseLLMClient:
        """
        LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        
        Args:
            provider: LLM ì œê³µì—…ì²´ ('anthropic', 'openai' ë“±)
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬ (api_key ë“±)
            
        Returns:
            BaseLLMClient ì¸ìŠ¤í„´ìŠ¤ (í”„ë¡¬í”„íŠ¸ ì¤‘ì•™ ê´€ë¦¬ ì§€ì›)
            
        Raises:
            ValueError: ì§€ì›í•˜ì§€ ì•ŠëŠ” í”„ë¡œë°”ì´ë”ì¸ ê²½ìš°
        """
        providers = {
            "anthropic": AnthropicLLMClient,
            # í–¥í›„ ì¶”ê°€ ê°€ëŠ¥: "openai": OpenAILLMClient
        }
        
        if provider not in providers:
            supported = ", ".join(providers.keys())
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM í”„ë¡œë°”ì´ë”: {provider}. ì§€ì› ëª©ë¡: {supported}")
        
        try:
            client_class = providers[provider]
            client = client_class(config["api_key"])
            logger.info(f"âœ… {provider} LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì™„ë£Œ (í”„ë¡¬í”„íŠ¸ ì¤‘ì•™ ê´€ë¦¬)")
            return client
        except Exception as e:
            logger.error(f"âŒ {provider} LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise
