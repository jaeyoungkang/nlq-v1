# backend/utils/bigquery/conversation_service.py
"""
BigQuery ëŒ€í™” ì„œë¹„ìŠ¤
ëŒ€í™” ì €ì¥/ì¡°íšŒ/ì‚­ì œ - ìŠ¤í‚¤ë§ˆ ì •ë¦¬ ê³„íš ì ìš©
"""

import os
import json
import logging
import uuid
from datetime import datetime, timezone
from typing import Dict, Any, List
from google.cloud import bigquery
from google.cloud.exceptions import NotFound

logger = logging.getLogger(__name__)

class ConversationService:
    """BigQuery ëŒ€í™” ê´€ë¦¬ ì„œë¹„ìŠ¤ - ìŠ¤í‚¤ë§ˆ ì •ë¦¬ ê³„íš ì ìš©"""
    
    def __init__(self, project_id: str, location: str = "asia-northeast3"):
        """
        ConversationService ì´ˆê¸°í™”
        
        Args:
            project_id: Google Cloud í”„ë¡œì íŠ¸ ID
            location: BigQuery ë¦¬ì „
        """
        self.project_id = project_id
        self.location = location
        try:
            self.client = bigquery.Client(project=project_id, location=location)
            logger.info(f"BigQuery ConversationService ì´ˆê¸°í™” ì™„ë£Œ: {project_id}")
        except Exception as e:
            logger.error(f"BigQuery ConversationService ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            raise
    
    def save_conversation(self, conversation_data: Dict[str, Any]) -> Dict[str, Any]:
        """ëŒ€í™” ë‚´ìš©ì„ BigQueryì— ì €ì¥"""
        try:
            dataset_name = os.getenv('CONVERSATION_DATASET', 'v1')
            table_id = f"{self.project_id}.{dataset_name}.conversations"
            
            # í•„ìˆ˜ í•„ë“œ ê²€ì¦
            required_fields = ['message_id', 'message_type', 'user_id']
            if any(field not in conversation_data for field in required_fields):
                raise ValueError(f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {required_fields}")

            # ìƒˆë¡œìš´ ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ë°ì´í„° ì •ë¦¬
            clean_data = self._clean_conversation_data(conversation_data)
            
            # í…Œì´ë¸” ì¡´ì¬ í™•ì¸ ë° ìƒì„±
            table_check_result = self._ensure_table_exists(dataset_name, 'conversations', self._create_conversations_table)
            if not table_check_result['success']:
                return {"success": False, "error": table_check_result['error']}

            # ë°ì´í„° ì‚½ì… - TableReference ì‚¬ìš©
            table_ref = self.client.dataset(dataset_name).table('conversations')
            errors = self.client.insert_rows_json(table_ref, [clean_data])
            
            if errors:
                logger.error(f"ëŒ€í™” ì €ì¥ ì‹¤íŒ¨: {errors}")
                return {"success": False, "error": f"ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {errors[0]}"}
            
            logger.info(f"ğŸ’¾ ëŒ€í™” ì €ì¥ ì™„ë£Œ: {clean_data['message_id']} - {clean_data['message_type']}")
            return {"success": True, "message": "ëŒ€í™”ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."}
            
        except Exception as e:
            logger.error(f"ëŒ€í™” ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {"success": False, "error": str(e)}

    def save_query_result(self, query_id: str, result_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì¿¼ë¦¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ 'query_results' í…Œì´ë¸”ì— ì €ì¥"""
        try:
            dataset_name = os.getenv('CONVERSATION_DATASET', 'v1')
            table_id = f"{self.project_id}.{dataset_name}.query_results"

            # í…Œì´ë¸” ì¡´ì¬ í™•ì¸ ë° ìƒì„±
            table_check_result = self._ensure_table_exists(dataset_name, 'query_results', self._create_query_results_table)
            if not table_check_result['success']:
                return {"success": False, "error": table_check_result['error']}

            # result_payload JSON ê°ì²´ ìƒì„±
            payload = {
                "status": "success" if result_data.get("success", False) else "error",
                "metadata": {
                    "row_count": result_data.get("row_count"),
                    "data_size_kb": len(json.dumps(result_data.get("data", []))) / 1024,
                    "is_summary": len(result_data.get("data", [])) < result_data.get("row_count", 0),
                    "schema": [{"name": k, "type": str(type(v).__name__)} for k, v in result_data.get("data", [{}])[0].items()] if result_data.get("data") else []
                },
                "data": result_data.get("data", []),
                "error": result_data.get("error")
            }

            # ì‚½ì…í•  í–‰ ë°ì´í„°
            row_to_insert = {
                "query_id": query_id,
                "result_payload": json.dumps(payload, default=str),
                "creation_time": datetime.now(timezone.utc).isoformat()
            }

            # TableReference ì‚¬ìš©
            table_ref = self.client.dataset(dataset_name).table('query_results')
            errors = self.client.insert_rows_json(table_ref, [row_to_insert])
            if errors:
                logger.error(f"ì¿¼ë¦¬ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {errors}")
                return {"success": False, "error": f"ì¿¼ë¦¬ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {errors[0]}"}

            logger.info(f"ğŸ“Š ì¿¼ë¦¬ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {query_id}")
            return {"success": True, "query_id": query_id}

        except Exception as e:
            logger.error(f"âŒ ì¿¼ë¦¬ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {"success": False, "error": f"ì¿¼ë¦¬ ê²°ê³¼ ì €ì¥ ì˜¤ë¥˜: {str(e)}"}

            
    def get_latest_conversation(self, user_id: str) -> Dict[str, Any]:
        """ì‚¬ìš©ìì˜ ëª¨ë“  ëŒ€í™” ê¸°ë¡ì„ ì‹œê°„ìˆœìœ¼ë¡œ ë³‘í•©í•˜ì—¬ ë°˜í™˜ (í…Œì´ë¸” ë¶€ì¬ ì‹œ ì˜ˆì™¸ ì²˜ë¦¬ ì¶”ê°€)"""
        try:
            dataset_name = os.getenv('CONVERSATION_DATASET', 'v1')
            conv_table = f"{self.project_id}.{dataset_name}.conversations"
            
            # í…Œì´ë¸” ì¡´ì¬ í™•ì¸ ë° ìƒì„±
            table_check_result = self._ensure_table_exists(dataset_name, 'conversations', self._create_conversations_table)
            if not table_check_result['success']:
                return {"success": True, "conversation": None, "message": "No conversations found."}
            
            # 1. ì‚¬ìš©ìì˜ ëª¨ë“  ë©”ì‹œì§€ë¥¼ ì‹œê°„ìˆœìœ¼ë¡œ ì¡°íšŒ
            query = f"""
            SELECT 
                c.message_id, c.message, c.message_type, c.timestamp,
                c.generated_sql, c.query_id
            FROM `{conv_table}` AS c
            WHERE c.user_id = @user_id
            ORDER BY c.timestamp ASC
            """
            job_config = bigquery.QueryJobConfig(query_parameters=[
                bigquery.ScalarQueryParameter("user_id", "STRING", user_id)
            ])
            
            rows = list(self.client.query(query, job_config=job_config).result())
            
            if not rows:
                return {"success": True, "conversation": None, "message": "No conversations found."}

            # ... (ì´í•˜ ë¡œì§ì€ ë™ì¼) ...
            
            query_ids = list(set([row.query_id for row in rows if row.query_id]))
            query_results_map = self._get_query_results_by_ids(query_ids, dataset_name)

            messages = []
            for row in rows:
                message_data = {
                    "message_id": row.message_id,
                    "message": row.message or "",
                    "message_type": row.message_type,
                    "timestamp": row.timestamp.isoformat() if row.timestamp else None,
                    "generated_sql": row.generated_sql,
                }
                
                if row.query_id in query_results_map:
                    payload = query_results_map[row.query_id]
                    if payload.get("status") == "success":
                        message_data['query_result_data'] = payload.get('data')
                        message_data['query_row_count'] = payload.get('metadata', {}).get('row_count')

                messages.append(message_data)
            
            return {
                "success": True,
                "conversation": {
                    "messages": messages,
                    "message_count": len(messages)
                }
            }

        except NotFound:
            # conversations í…Œì´ë¸”ì´ ì—†ì„ ê²½ìš°ì˜ ì²˜ë¦¬
            logger.warning(f"í…Œì´ë¸” '{conv_table}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ ëŒ€í™” ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return {"success": True, "conversation": None, "message": "No conversations found."}
        except Exception as e:
            logger.error(f"ì „ì²´ ëŒ€í™” ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {"success": False, "error": str(e), "messages": []}

    def get_conversation_context(self, user_id: str, max_messages: int = 10) -> Dict[str, Any]:
        """LLM ì»¨í…ìŠ¤íŠ¸ìš© ëŒ€í™” ê¸°ë¡ ì¡°íšŒ - conversation_id ì—†ì´ user_idë§Œìœ¼ë¡œ"""
        try:
            dataset_name = os.getenv('CONVERSATION_DATASET', 'v1')
            conv_table = f"{self.project_id}.{dataset_name}.conversations"
            
            # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
            table_check_result = self._ensure_table_exists(dataset_name, 'conversations', self._create_conversations_table)
            if not table_check_result['success']:
                return {"success": True, "context": [], "context_length": 0}
            
            # ìµœê·¼ ë©”ì‹œì§€ë“¤ì„ ì‹œê°„ìˆœìœ¼ë¡œ ì¡°íšŒ (ìµœëŒ€ max_messagesê°œ)
            query = f"""
            SELECT 
                message_id, message, message_type, timestamp,
                generated_sql, query_id
            FROM `{conv_table}`
            WHERE user_id = @user_id
            ORDER BY timestamp DESC
            LIMIT @max_messages
            """
            
            job_config = bigquery.QueryJobConfig(query_parameters=[
                bigquery.ScalarQueryParameter("user_id", "STRING", user_id),
                bigquery.ScalarQueryParameter("max_messages", "INT64", max_messages)
            ])
            
            rows = list(self.client.query(query, job_config=job_config).result())
            
            # ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬ (ì˜¤ë˜ëœ ê²ƒë¶€í„°)
            rows.reverse()
            
            # ì¿¼ë¦¬ ê²°ê³¼ ì¼ê´„ ì¡°íšŒ
            query_ids = list(set([row.query_id for row in rows if row.query_id]))
            query_results_map = self._get_query_results_by_ids(query_ids, dataset_name)
            
            context_messages = []
            for row in rows:
                message_data = {
                    "role": "user" if row.message_type == "user" else "assistant",
                    "content": row.message or "",
                    "timestamp": row.timestamp.isoformat() if row.timestamp else None,
                    "metadata": {
                        "message_type": row.message_type,
                        "generated_sql": row.generated_sql,
                        "query_id": row.query_id
                    }
                }
                
                # ì¿¼ë¦¬ ê²°ê³¼ í¬í•¨
                if row.query_id and row.query_id in query_results_map:
                    payload = query_results_map[row.query_id]
                    if payload.get("status") == "success":
                        message_data['query_result_data'] = payload.get('data')
                        message_data['query_row_count'] = payload.get('metadata', {}).get('row_count')
                        logger.info(f"ğŸ“Š ì»¨í…ìŠ¤íŠ¸ì— ì¿¼ë¦¬ ê²°ê³¼ í¬í•¨: query_id={row.query_id}, {len(payload.get('data', []))}í–‰")

                context_messages.append(message_data)
            
            logger.info(f"ğŸ“š ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ ì™„ë£Œ: {len(context_messages)}ê°œ ë©”ì‹œì§€ (user_id: {user_id})")
            if len(context_messages) > 0:
                logger.info(f"ğŸ“š ì»¨í…ìŠ¤íŠ¸ ìƒ˜í”Œ: {context_messages[-1]}")
            
            return {
                "success": True,
                "context": context_messages,
                "context_length": len(context_messages)
            }
            
        except Exception as e:
            logger.error(f"ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {"success": False, "error": str(e), "context": [], "context_length": 0}

    def _get_query_results_by_ids(self, query_ids: List[str], dataset_name: str) -> Dict[str, Any]:
        """ID ëª©ë¡ìœ¼ë¡œ ì¿¼ë¦¬ ê²°ê³¼ í˜ì´ë¡œë“œ ì¡°íšŒ"""
        if not query_ids:
            return {}
        
        results_table = f"{self.project_id}.{dataset_name}.query_results"
        query = f"""
            SELECT query_id, result_payload
            FROM `{results_table}`
            WHERE query_id IN UNNEST(@query_ids)
        """
        job_config = bigquery.QueryJobConfig(query_parameters=[
            bigquery.ArrayQueryParameter("query_ids", "STRING", query_ids)
        ])
        
        rows = self.client.query(query, job_config=job_config).result()
        
        results_map = {}
        for row in rows:
            try:
                results_map[row.query_id] = json.loads(row.result_payload)
            except (json.JSONDecodeError, TypeError):
                logger.warning(f"ê²°ê³¼ í˜ì´ë¡œë“œ íŒŒì‹± ì‹¤íŒ¨: query_id={row.query_id}")
                results_map[row.query_id] = {"error": "payload parsing failed"}
        return results_map

    def _clean_conversation_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ì €ì¥ì„ ìœ„í•´ ëŒ€í™” ë°ì´í„° ì •ë¦¬"""
        return {
            'message_id': data.get('message_id'),
            'user_id': data.get('user_id'),
            'message_type': data.get('message_type'),
            'message': data.get('message'),
            'timestamp': data.get('timestamp', datetime.now(timezone.utc).isoformat()),
            'generated_sql': data.get('generated_sql'),
            'query_id': data.get('query_id')
        }

    def _ensure_table_exists(self, dataset_name: str, table_name: str, create_method: callable) -> Dict[str, Any]:
        """í…Œì´ë¸” ì¡´ì¬ í™•ì¸ ë° ìƒì„± í—¬í¼"""
        table_ref = self.client.dataset(dataset_name).table(table_name)
        try:
            self.client.get_table(table_ref)
            return {"success": True, "action": "exists"}
        except NotFound:
            logger.info(f"í…Œì´ë¸” {table_name} ì—†ìŒ. ìƒì„± ì‹œë„.")
            result = create_method(dataset_name)
            # ë™ì‹œ ìƒì„±ìœ¼ë¡œ ì¸í•œ Already Exists ì—ëŸ¬ëŠ” ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
            if not result['success'] and 'Already Exists' in result.get('error', ''):
                logger.info(f"í…Œì´ë¸” {table_name} ì´ë¯¸ ì¡´ì¬í•¨ (ë™ì‹œ ìƒì„±ë¨)")
                return {"success": True, "action": "exists"}
            return result

    def _create_conversations_table(self, dataset_name: str) -> Dict[str, Any]:
        """ìƒˆë¡œìš´ ìŠ¤í‚¤ë§ˆë¡œ conversations í…Œì´ë¸” ìƒì„±"""
        table_id = f"{self.project_id}.{dataset_name}.conversations"
        try:
            schema = [
                bigquery.SchemaField("message_id", "STRING", mode="REQUIRED"),
                bigquery.SchemaField("user_id", "STRING", mode="REQUIRED"),
                bigquery.SchemaField("message_type", "STRING", mode="REQUIRED"),
                bigquery.SchemaField("message", "STRING"),
                bigquery.SchemaField("timestamp", "TIMESTAMP", mode="REQUIRED"),
                bigquery.SchemaField("generated_sql", "STRING"),
                bigquery.SchemaField("query_id", "STRING"),
            ]
            table = bigquery.Table(table_id, schema=schema)
            table.time_partitioning = bigquery.TimePartitioning(type_=bigquery.TimePartitioningType.DAY, field="timestamp")
            self.client.create_table(table)
            logger.info(f"í…Œì´ë¸” ìƒì„± ì™„ë£Œ: {table_id}")
            return {"success": True, "action": "created"}
        except Exception as e:
            logger.error(f"í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨ {table_id}: {e}")
            return {"success": False, "error": str(e)}

    def _create_query_results_table(self, dataset_name: str) -> Dict[str, Any]:
        """ìƒˆë¡œìš´ ìŠ¤í‚¤ë§ˆë¡œ query_results í…Œì´ë¸” ìƒì„±"""
        table_id = f"{self.project_id}.{dataset_name}.query_results"
        try:
            schema = [
                bigquery.SchemaField("query_id", "STRING", mode="REQUIRED"),
                bigquery.SchemaField("result_payload", "STRING"), # JSON as STRING
                bigquery.SchemaField("creation_time", "TIMESTAMP", mode="REQUIRED"),
            ]
            table = bigquery.Table(table_id, schema=schema)
            table.time_partitioning = bigquery.TimePartitioning(type_=bigquery.TimePartitioningType.DAY, field="creation_time")
            self.client.create_table(table)
            logger.info(f"í…Œì´ë¸” ìƒì„± ì™„ë£Œ: {table_id}")
            return {"success": True, "action": "created"}
        except Exception as e:
            logger.error(f"í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨ {table_id}: {e}")
            return {"success": False, "error": str(e)}

    def get_user_conversations(self, user_id: str, limit: int = 50, offset: int = 0) -> Dict[str, Any]:
        """ì‚¬ìš©ìì˜ ëŒ€í™” íˆìŠ¤í† ë¦¬ ëª©ë¡ ì¡°íšŒ - ë‹¨ì¼ ì“°ë ˆë“œ ëª¨ë¸"""
        try:
            dataset_name = os.getenv('CONVERSATION_DATASET', 'v1')
            conversations_table = f"{self.project_id}.{dataset_name}.conversations"
            
            table_check_result = self._ensure_table_exists(dataset_name, 'conversations', self._create_conversations_table)
            if not table_check_result['success']:
                return {"success": True, "conversations": [], "count": 0}
            
            query = f"""
            SELECT 
                MIN(timestamp) as start_time,
                MAX(timestamp) as last_time,
                COUNT(*) as message_count,
                ARRAY_AGG(
                    CASE WHEN message_type = 'user' THEN message ELSE NULL END IGNORE NULLS
                    ORDER BY timestamp 
                    LIMIT 1
                )[SAFE_OFFSET(0)] as first_message
            FROM `{conversations_table}`
            WHERE user_id = @user_id
            """
            job_config = bigquery.QueryJobConfig(query_parameters=[
                bigquery.ScalarQueryParameter("user_id", "STRING", user_id)
            ])
            
            results = self.client.query(query, job_config=job_config).result()
            row = next(iter(results), None)
            
            conversations = []
            if row and row.message_count > 0:
                conversations = [{
                    "user_id": user_id,
                    "start_time": row.start_time.isoformat() if row.start_time else None,
                    "last_time": row.last_time.isoformat() if row.last_time else None,
                    "message_count": row.message_count,
                    "first_message": row.first_message or "ëŒ€í™” ì—†ìŒ"
                }]
            
            return {"success": True, "conversations": conversations, "count": len(conversations)}
        except Exception as e:
            logger.error(f"ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {"success": False, "error": str(e), "conversations": []}