"""
í†µí•© LLM í´ë¼ì´ì–¸íŠ¸ - í”„ë¡¬í”„íŠ¸ ì¤‘ì•™ ê´€ë¦¬ ì‹œìŠ¤í…œ ì ìš©
ë¦¬íŒ©í† ë§: í•˜ë“œì½”ë”©ëœ í”„ë¡¬í”„íŠ¸ë¥¼ JSON íŒŒì¼ ê¸°ë°˜ ì‹œìŠ¤í…œìœ¼ë¡œ êµì²´
"""

import json
import logging
import re
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any
import anthropic

# í”„ë¡¬í”„íŠ¸ ì¤‘ì•™ ê´€ë¦¬ ì‹œìŠ¤í…œ ì„í¬íŠ¸
from .prompts import prompt_manager

logger = logging.getLogger(__name__)


class BaseLLMClient(ABC):
    """LLM í´ë¼ì´ì–¸íŠ¸ ì¶”ìƒ ë² ì´ìŠ¤ í´ë˜ìŠ¤"""
    
    @abstractmethod
    def classify_input(self, user_input: str, conversation_context: List[Dict] = None) -> dict:
        """ì‚¬ìš©ì ì…ë ¥ ë¶„ë¥˜"""
        pass
    
    @abstractmethod
    def generate_sql(self, question: str, project_id: str, dataset_ids: list = None, 
                   conversation_context: List[Dict] = None) -> dict:
        """SQL ìƒì„±"""
        pass
    
    @abstractmethod
    def analyze_data(self, question: str, previous_data: list = None, previous_sql: str = None, 
                   conversation_context: List[Dict] = None) -> dict:
        """ë°ì´í„° ë¶„ì„"""
        pass
    
    @abstractmethod
    def generate_guide(self, question: str, context: str = "") -> dict:
        """ê°€ì´ë“œ ìƒì„±"""
        pass
    
    @abstractmethod
    def generate_metadata_response(self, question: str, metadata: dict) -> dict:
        """ë©”íƒ€ë°ì´í„° ì‘ë‹µ ìƒì„±"""
        pass
    
    @abstractmethod
    def generate_out_of_scope(self, question: str) -> dict:
        """ë²”ìœ„ ì™¸ ì‘ë‹µ ìƒì„±"""
        pass


class AnthropicLLMClient(BaseLLMClient):
    """Anthropic Claude LLM í´ë¼ì´ì–¸íŠ¸ - í”„ë¡¬í”„íŠ¸ ì¤‘ì•™ ê´€ë¦¬ ë²„ì „"""
    
    def __init__(self, api_key: str):
        """
        Anthropic í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        
        Args:
            api_key: Anthropic API í‚¤
        """
        self.api_key = api_key
        try:
            self.client = anthropic.Anthropic(api_key=api_key)
            logger.info("âœ… Anthropic LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ (í”„ë¡¬í”„íŠ¸ ì¤‘ì•™ ê´€ë¦¬)")
        except Exception as e:
            logger.error(f"âŒ Anthropic í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            raise
    
    def classify_input(self, user_input: str, conversation_context: List[Dict] = None) -> dict:
        """
        ì‚¬ìš©ì ì…ë ¥ì„ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜ (í†µí•© ì•„í‚¤í…ì²˜ ì ìš©)
        
        Args:
            user_input: ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
            conversation_context: ì´ì „ ëŒ€í™” ê¸°ë¡ (ì„ íƒì‚¬í•­)
            
        Returns:
            ë¶„ë¥˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        return self._execute_unified_prompting(
            category='classification',
            input_data={'user_input': user_input},
            conversation_context=conversation_context
        )
    
    
    def _normalize_conversation_context(self, conversation_context: List[Dict] = None) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ì •ê·œí™” - í•­ìƒ ë¬¸ìì—´ ë°˜í™˜"""
        if not conversation_context or len(conversation_context) == 0:
            return "[ì´ì „ ëŒ€í™” ì—†ìŒ]"
        
        return self._format_conversation_context(conversation_context)
    
    def _calculate_dynamic_tokens(self, category: str, context: str) -> int:
        """
        ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ì™€ ì¹´í…Œê³ ë¦¬ì— ë”°ë¥¸ ì§€ëŠ¥í˜• í† í° í• ë‹¹
        """
        # ê¸°ë³¸ í† í° ì„¤ì •
        base_tokens = {
            'classification': 300,     # ë¶„ë¥˜ëŠ” ì§§ì€ JSON ì‘ë‹µ
            'sql_generation': 1200,    # SQLì€ ë³µì¡í•œ ì¿¼ë¦¬ ê°€ëŠ¥
            'data_analysis': 1200,     # ë¶„ì„ì€ ìƒì„¸í•œ ì„¤ëª… í•„ìš”
            'guide_request': 800,      # ê°€ì´ë“œëŠ” ì¤‘ê°„ ê¸¸ì´
            'metadata_request': 600    # ë©”íƒ€ë°ì´í„°ëŠ” êµ¬ì¡°í™”ëœ ì‘ë‹µ
        }
        
        base = base_tokens.get(category, 400)
        
        # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë”°ë¥¸ ì¶”ê°€ í† í°
        if context == "[ì´ì „ ëŒ€í™” ì—†ìŒ]":
            return base
        
        # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì¸¡ì • (ëŒ€ëµì )
        context_length = len(context.split())
        
        if context_length < 50:
            context_bonus = 50      # ì§§ì€ ì»¨í…ìŠ¤íŠ¸
        elif context_length < 200:
            context_bonus = 100     # ì¤‘ê°„ ì»¨í…ìŠ¤íŠ¸  
        else:
            context_bonus = 200     # ê¸´ ì»¨í…ìŠ¤íŠ¸
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì»¨í…ìŠ¤íŠ¸ ë¯¼ê°ë„
        context_multiplier = {
            'classification': 0.5,    # ë¶„ë¥˜ëŠ” ì»¨í…ìŠ¤íŠ¸ ì˜í–¥ ì ìŒ
            'sql_generation': 1.0,    # SQLì€ ì»¨í…ìŠ¤íŠ¸ ì¤‘ìš”
            'data_analysis': 1.5,     # ë¶„ì„ì€ ì»¨í…ìŠ¤íŠ¸ ë§¤ìš° ì¤‘ìš”
            'guide_request': 0.7,
            'metadata_request': 0.3
        }
        
        multiplier = context_multiplier.get(category, 1.0)
        final_bonus = int(context_bonus * multiplier)
        
        return min(base + final_bonus, 2000)  # ìµœëŒ€ 2000 í† í° ì œí•œ

    def _execute_unified_prompting(self, 
                                 category: str,
                                 input_data: Dict[str, Any],
                                 conversation_context: List[Dict] = None) -> dict:
        """
        í†µí•© í”„ë¡¬í”„íŒ… ì‹¤í–‰ - í•­ìƒ ê°™ì€ í…œí”Œë¦¿ ì‚¬ìš©
        
        Args:
            category: í”„ë¡¬í”„íŠ¸ ì¹´í…Œê³ ë¦¬
            input_data: ì…ë ¥ ë°ì´í„°
            conversation_context: ì´ì „ ëŒ€í™” ê¸°ë¡
            
        Returns:
            LLM ì‘ë‹µ ê²°ê³¼
        """
        try:
            # ì»¨í…ìŠ¤íŠ¸ ì •ê·œí™”
            normalized_context = self._normalize_conversation_context(conversation_context)
            logger.info(f"ğŸ”§ í†µí•© í”„ë¡¬í”„íŒ… ì‹¤í–‰: category={category}, context={'ìˆìŒ' if normalized_context != '[ì´ì „ ëŒ€í™” ì—†ìŒ]' else 'ì—†ìŒ'}")
            
            # ë‹¨ì¼ í…œí”Œë¦¿ ì‚¬ìš©
            system_prompt = prompt_manager.get_prompt(
                category=category,
                template_name='system_prompt',
                **input_data,
                fallback_prompt=self._get_fallback_system_prompt(category)
            )
            
            user_prompt = prompt_manager.get_prompt(
                category=category,
                template_name='user_prompt',
                conversation_context=normalized_context,
                **input_data,
                fallback_prompt=self._get_fallback_user_prompt(category, input_data)
            )
            
            # ë™ì  í† í° í• ë‹¹
            max_tokens = self._calculate_dynamic_tokens(category, normalized_context)
            logger.info(f"ğŸ”§ ë™ì  í† í° í• ë‹¹: {max_tokens}í† í° (category={category})")
            
            # Claude API í˜¸ì¶œ
            response = self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=max_tokens,
                system=system_prompt,
                messages=[{"role": "user", "content": user_prompt}]
            )
            
            response_text = response.content[0].text.strip()
            return self._post_process_response(category, response_text, normalized_context)
            
        except (anthropic.RateLimitError, anthropic.APIStatusError) as e:
            logger.warning(f"âš ï¸ Anthropic API ì†ë„ ì œí•œ ë˜ëŠ” ê³¼ë¶€í•˜: {str(e)}")
            return {
                "success": False,
                "error": "AI ì„œë¹„ìŠ¤ê°€ ì¼ì‹œì ìœ¼ë¡œ ìš”ì²­ì´ ë§ì•„ ì‘ë‹µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "error_type": "rate_limit_error"
            }
        except Exception as e:
            logger.error(f"âŒ í†µí•© í”„ë¡¬í”„íŒ… ì˜¤ë¥˜ ({category}): {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }

    def _format_conversation_context(self, context: List[Dict]) -> str:
        """ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ LLM í”„ë¡¬í”„íŠ¸ìš© í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        if not context:
            return ""
        
        formatted_lines = []
        for msg in context[-3:]:  # ìµœê·¼ 5ê°œ ë©”ì‹œì§€ë§Œ ì‚¬ìš©
            role = "ì‚¬ìš©ì" if msg['role'] == "user" else "AI"
            timestamp = msg.get('timestamp', '')[:19] if msg.get('timestamp') else ''
            content = msg['content'][:200] + "..." if len(msg['content']) > 200 else msg['content']
            
            # SQL ì •ë³´ê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€
            sql_info = ""
            if msg.get('metadata', {}).get('generated_sql'):
                sql_info = f" [SQL ìƒì„±í•¨]"
            
            formatted_lines.append(f"[{timestamp}] {role}: {content}{sql_info}")
        
        return "\n".join(formatted_lines)
    
    # ì¹´í…Œê³ ë¦¬ë³„ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ê¸°ë“¤
    def _process_classification_context(self, context: List[Dict]) -> Dict[str, Any]:
        """ë¶„ë¥˜ìš© ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬"""
        return {'conversation_context': self._format_conversation_context(context)}
    
    def _process_sql_context(self, context: List[Dict]) -> Dict[str, Any]:
        """SQL ìƒì„±ìš© ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬"""
        return {
            'conversation_context': self._format_conversation_context(context),
            'previous_sqls': self._extract_sql_patterns(context),
            'frequently_used_tables': self._extract_table_usage(context)
        }
    
    def _process_analysis_context(self, context: List[Dict]) -> Dict[str, Any]:
        """ë¶„ì„ìš© ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬"""
        return {
            'conversation_context': self._format_conversation_context(context),
            'previous_analysis': self._extract_previous_analysis(context)
        }
    
    def _extract_sql_patterns(self, context: List[Dict]) -> str:
        """ì´ì „ ëŒ€í™”ì—ì„œ SQL íŒ¨í„´ ì¶”ì¶œ"""
        sql_patterns = []
        for msg in context:
            if msg.get('metadata', {}).get('generated_sql'):
                sql = msg['metadata']['generated_sql']
                if sql and len(sql) > 20:
                    sql_patterns.append(sql[:100] + "...")
        
        return "\n".join(sql_patterns) if sql_patterns else "ì´ì „ SQL ì—†ìŒ"
    
    def _extract_table_usage(self, context: List[Dict]) -> str:
        """ìì£¼ ì‚¬ìš©ë˜ëŠ” í…Œì´ë¸” íŒ¨í„´ ì¶”ì¶œ"""
        tables = []
        for msg in context:
            if msg.get('metadata', {}).get('generated_sql'):
                sql = msg['metadata']['generated_sql']
                if sql and 'FROM' in sql.upper():
                    # ê°„ë‹¨í•œ í…Œì´ë¸”ëª… ì¶”ì¶œ
                    import re
                    table_matches = re.findall(r'FROM\s+`([^`]+)`', sql, re.IGNORECASE)
                    tables.extend(table_matches)
        
        unique_tables = list(set(tables))
        return ", ".join(unique_tables) if unique_tables else "ê¸°ë³¸ í…Œì´ë¸” ì‚¬ìš©"
    
    def _extract_previous_analysis(self, context: List[Dict]) -> str:
        """ì´ì „ ë¶„ì„ ê²°ê³¼ ì¶”ì¶œ"""
        analyses = []
        for msg in context:
            if msg['role'] == 'assistant' and 'ë¶„ì„' in msg['content']:
                analyses.append(msg['content'][:150] + "...")
        
        return "\n".join(analyses) if analyses else "ì´ì „ ë¶„ì„ ì—†ìŒ"
    
    def _post_process_response(self, category: str, response_text: str, normalized_context: str) -> dict:
        """ì¹´í…Œê³ ë¦¬ë³„ ì‘ë‹µ í›„ì²˜ë¦¬"""
        if category == 'classification':
            try:
                classification = json.loads(response_text)
                if all(key in classification for key in ["category", "confidence"]):
                    context_info = f" (ì»¨í…ìŠ¤íŠ¸: ìˆìŒ)" if normalized_context != "[ì´ì „ ëŒ€í™” ì—†ìŒ]" else " (ì»¨í…ìŠ¤íŠ¸: ì—†ìŒ)"
                    logger.info(f"ğŸ¯ í†µí•© ë¶„ë¥˜: {classification['category']}{context_info}")
                    return {"success": True, "classification": classification}
                else:
                    raise ValueError("í•„ìˆ˜ í•„ë“œ ëˆ„ë½")
            except (json.JSONDecodeError, ValueError):
                return {
                    "success": True,
                    "classification": {
                        "category": "query_request",
                        "confidence": 0.5,
                        "reasoning": "í†µí•© ë¶„ë¥˜ íŒŒì‹± ì‹¤íŒ¨ë¡œ ê¸°ë³¸ê°’ ì‚¬ìš©"
                    }
                }
        elif category == 'sql_generation':
            cleaned_sql = self._clean_sql_response(response_text)
            logger.info(f"ğŸ”§ í†µí•© SQL ìƒì„± ì™„ë£Œ: {cleaned_sql[:100]}...")
            return {
                "success": True,
                "sql": cleaned_sql,
                "raw_response": response_text
            }
        elif category == 'data_analysis':
            logger.info(f"ğŸ” í†µí•© ë°ì´í„° ë¶„ì„ ì™„ë£Œ")
            return {
                "success": True,
                "analysis": response_text
            }
        else:
            return {
                "success": True,
                "response": response_text
            }
    
    def _get_fallback_system_prompt(self, category: str) -> str:
        """ì¹´í…Œê³ ë¦¬ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ Fallback"""
        fallbacks = {
            'classification': self._get_fallback_classification_prompt(),
            'sql_generation': "BigQuery SQL ì „ë¬¸ê°€ë¡œì„œ ìì—°ì–´ë¥¼ SQLë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.",
            'data_analysis': "ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ë¡œì„œ ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”."
        }
        return fallbacks.get(category, "ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ë¡œì„œ ë„ì›€ì„ ì œê³µí•´ì£¼ì„¸ìš”.")
    
    def _get_fallback_user_prompt(self, category: str, input_data: Dict[str, Any]) -> str:
        """ì¹´í…Œê³ ë¦¬ë³„ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ Fallback"""
        if category == 'classification':
            return f"ë¶„ë¥˜í•  ì…ë ¥: {input_data.get('user_input', '')}"
        elif category == 'sql_generation':
            return f"SQL ìƒì„± ì§ˆë¬¸: {input_data.get('question', '')}"
        elif category == 'data_analysis':
            return f"ë¶„ì„ ì§ˆë¬¸: {input_data.get('question', '')}"
        else:
            return str(input_data)
    
    def generate_sql(self, question: str, project_id: str, dataset_ids: list = None, 
                   conversation_context: List[Dict] = None) -> dict:
        """
        ìì—°ì–´ ì§ˆë¬¸ì„ BigQuery SQLë¡œ ë³€í™˜ (í†µí•© ì•„í‚¤í…ì²˜ ì ìš©)
        
        Args:
            question: ì‚¬ìš©ìì˜ ìì—°ì–´ ì§ˆë¬¸
            project_id: BigQuery í”„ë¡œì íŠ¸ ID  
            dataset_ids: ì‚¬ìš©í•  ë°ì´í„°ì…‹ ID ëª©ë¡ (ì„ íƒì‚¬í•­)
            conversation_context: ì´ì „ ëŒ€í™” ê¸°ë¡ (ì„ íƒì‚¬í•­)
            
        Returns:
            SQL ìƒì„± ê²°ê³¼
        """
        default_table = "`nlq-ex.test_dataset.events_20210131`"
        
        return self._execute_unified_prompting(
            category='sql_generation',
            input_data={
                'question': question,
                'project_id': project_id,
                'default_table': default_table
            },
            conversation_context=conversation_context
        )
    
    def generate_metadata_response(self, question: str, metadata: dict) -> dict:
        """
        ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì‘ë‹µ ìƒì„± (í”„ë¡¬í”„íŠ¸ ì¤‘ì•™ ê´€ë¦¬ ì ìš©)
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            metadata: BigQuery í…Œì´ë¸” ë©”íƒ€ë°ì´í„°
            
        Returns:
            ë©”íƒ€ë°ì´í„° ì‘ë‹µ ê²°ê³¼
        """
        try:
            table_info = metadata.get('table_info', {})
            schema = metadata.get('schema', [])
            
            # ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            schema_text = ""
            if schema:
                schema_text = "\n".join([
                    f"- {field['name']} ({field['type']}): {field.get('description', 'ì„¤ëª… ì—†ìŒ')}"
                    for field in schema[:10]  # ìƒìœ„ 10ê°œë§Œ
                ])
            
            # í”„ë¡¬í”„íŠ¸ ì¤‘ì•™ ê´€ë¦¬ ì‹œìŠ¤í…œì—ì„œ ë¡œë“œ
            prompt = prompt_manager.get_prompt(
                category='data_analysis',
                template_name='metadata_response',
                table_id=table_info.get('table_id', 'nlq-ex.test_dataset.events_20210131'),
                row_count=f"{table_info.get('num_rows', 'N/A'):,}" if table_info.get('num_rows') else 'N/A',
                size_mb=table_info.get('size_mb', 'N/A'),
                created_date=table_info.get('created', 'N/A'),
                schema_text=schema_text,
                user_question=question,
                fallback_prompt=self._get_fallback_metadata_prompt(question, table_info, schema_text)
            )

            response = self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=800,
                messages=[{"role": "user", "content": prompt}]
            )
            
            response_text = response.content[0].text.strip()
            logger.info(f"ğŸ“‹ ë©”íƒ€ë°ì´í„° ì‘ë‹µ ìƒì„± ì™„ë£Œ (ì¤‘ì•™ ê´€ë¦¬)")
            
            return {
                "success": True,
                "response": response_text
            }
            
        except Exception as e:
            logger.error(f"âŒ ë©”íƒ€ë°ì´í„° ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }

    def analyze_data(self, question: str, previous_data: list = None, previous_sql: str = None, 
                   conversation_context: List[Dict] = None) -> dict:
        """
        ì¡°íšŒëœ ë°ì´í„°ì— ëŒ€í•œ ë¶„ì„ ìƒì„± (í†µí•© ì•„í‚¤í…ì²˜ ì ìš©)
        
        Args:
            question: ì‚¬ìš©ìì˜ ë¶„ì„ ìš”ì²­ ì§ˆë¬¸
            previous_data: ì´ì „ì— ì¡°íšŒëœ ë°ì´í„°
            previous_sql: ì´ì „ì— ì‹¤í–‰ëœ SQL
            conversation_context: ì´ì „ ëŒ€í™” ê¸°ë¡ (ì„ íƒì‚¬í•­)
            
        Returns:
            ë°ì´í„° ë¶„ì„ ê²°ê³¼
        """
        
        data_sample = previous_data[:5] if previous_data and len(previous_data) > 5 else previous_data
        
        return self._execute_unified_prompting(
            category='data_analysis',
            input_data={
                'question': question,
                'previous_sql': previous_sql or "N/A",
                'total_rows': len(previous_data) if previous_data else 0,
                'data_sample': json.dumps(data_sample, indent=2, ensure_ascii=False, default=str) if data_sample else "[]"
            },
            conversation_context=conversation_context
        )

    def generate_guide(self, question: str, context: str = "") -> dict:
        """
        ê°€ì´ë“œ ì‘ë‹µ ìƒì„± (í”„ë¡¬í”„íŠ¸ ì¤‘ì•™ ê´€ë¦¬ ì ìš©)
        
        Args:
            question: ì‚¬ìš©ìì˜ ê°€ì´ë“œ ìš”ì²­
            context: í˜„ì¬ ìƒí™© ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            ê°€ì´ë“œ ì‘ë‹µ ê²°ê³¼
        """
        try:
            # í”„ë¡¬í”„íŠ¸ ì¤‘ì•™ ê´€ë¦¬ ì‹œìŠ¤í…œì—ì„œ ë¡œë“œ
            guide_prompt = prompt_manager.get_prompt(
                category='guides',
                template_name='usage_guide',
                context=context,
                user_question=question,
                fallback_prompt=self._get_fallback_guide_prompt(question, context)
            )

            response = self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=1000,
                messages=[{"role": "user", "content": guide_prompt}]
            )
            
            guide = response.content[0].text.strip()
            logger.info(f"ğŸ’¡ ê°€ì´ë“œ ì‘ë‹µ ìƒì„± ì™„ë£Œ (ì¤‘ì•™ ê´€ë¦¬)")
            
            return {
                "success": True,
                "guide": guide
            }
            
        except Exception as e:
            logger.error(f"âŒ ê°€ì´ë“œ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "guide": None
            }
    
    def generate_out_of_scope(self, question: str) -> dict:
        """
        ê¸°ëŠ¥ ë²”ìœ„ ì™¸ ìš”ì²­ì— ëŒ€í•œ ì‘ë‹µ ìƒì„± (í”„ë¡¬í”„íŠ¸ ì¤‘ì•™ ê´€ë¦¬ ì ìš©)
        
        Args:
            question: ì‚¬ìš©ìì˜ ì§ˆë¬¸
            
        Returns:
            ë²”ìœ„ ì™¸ ì‘ë‹µ ê²°ê³¼
        """
        try:
            # í”„ë¡¬í”„íŠ¸ ì¤‘ì•™ ê´€ë¦¬ ì‹œìŠ¤í…œì—ì„œ ë¡œë“œ
            scope_prompt = prompt_manager.get_prompt(
                category='guides',
                template_name='out_of_scope',
                user_question=question,
                fallback_prompt=self._get_fallback_out_of_scope_prompt(question)
            )

            response = self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=400,
                messages=[{"role": "user", "content": scope_prompt}]
            )
            
            scope_response = response.content[0].text.strip()
            logger.info(f"ğŸš« ë²”ìœ„ ì™¸ ì‘ë‹µ ìƒì„± ì™„ë£Œ (ì¤‘ì•™ ê´€ë¦¬)")
            
            return {
                "success": True,
                "response": scope_response
            }
            
        except Exception as e:
            logger.error(f"âŒ ë²”ìœ„ ì™¸ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "response": None
            }
    
    def _create_sql_system_prompt_from_templates(self, project_id: str, dataset_ids: List[str] = None) -> str:
        """í”„ë¡¬í”„íŠ¸ ì¤‘ì•™ ê´€ë¦¬ ì‹œìŠ¤í…œì—ì„œ SQL ìƒì„± ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        default_table = "`nlq-ex.test_dataset.events_20210131`"
        
        try:
            # ê¸°ë³¸ ë°ì´í„°ì…‹ ì •ë³´ ìƒì„±
            dataset_info = prompt_manager.get_prompt(
                category='sql_generation',
                template_name='dataset_info_template',
                default_table=default_table,
                fallback_prompt=""
            )
            
            # ì¶”ê°€ ë°ì´í„°ì…‹ì´ ìˆëŠ” ê²½ìš°
            if dataset_ids:
                dataset_list = ", ".join([f"`{project_id}.{ds}`" for ds in dataset_ids])
                additional_datasets = prompt_manager.get_prompt(
                    category='sql_generation',
                    template_name='additional_datasets_template',
                    dataset_list=dataset_list,
                    fallback_prompt=""
                )
                dataset_info += additional_datasets
            
            # ë©”ì¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
            system_prompt = prompt_manager.get_prompt(
                category='sql_generation',
                template_name='system_prompt',
                project_id=project_id,
                dataset_info=dataset_info,
                default_table=default_table,
                fallback_prompt=self._get_fallback_sql_system_prompt(project_id, default_table)
            )
            
            return system_prompt
            
        except Exception as e:
            logger.error(f"âŒ SQL ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return self._get_fallback_sql_system_prompt(project_id, default_table)
    
    def _clean_sql_response(self, raw_response: str) -> str:
        """Claude ì‘ë‹µì—ì„œ SQL ì¿¼ë¦¬ë§Œ ì¶”ì¶œí•˜ê³  ì •ë¦¬ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)"""
        
        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
        if '```sql' in raw_response:
            sql_match = re.search(r'```sql\s*\n(.*?)\n```', raw_response, re.DOTALL)
            if sql_match:
                raw_response = sql_match.group(1)
        elif '```' in raw_response:
            code_match = re.search(r'```(.*?)```', raw_response, re.DOTALL)
            if code_match:
                raw_response = code_match.group(1)
        
        # ì •ë¦¬ ê³¼ì •
        sql_query = raw_response.strip()
        
        # ì£¼ì„ ì œê±° (SQL í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì§€ ì•Šì€ ë¼ì¸ë§Œ)
        lines = sql_query.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            # ì£¼ì„ì´ì§€ë§Œ SQL êµ¬ë¬¸ì´ í¬í•¨ëœ ê²½ìš°ëŠ” ìœ ì§€
            if line.startswith('--') and not any(
                keyword in line.upper() 
                for keyword in ['SELECT', 'FROM', 'WHERE', 'GROUP', 'ORDER', 'LIMIT']
            ):
                continue
            cleaned_lines.append(line)
        
        # ì¬ì¡°í•©
        sql_query = '\n'.join(cleaned_lines)
        
        # ì„¸ë¯¸ì½œë¡  ì¶”ê°€
        if not sql_query.rstrip().endswith(';'):
            sql_query = sql_query.rstrip() + ';'
        
        return sql_query

    # === ì¶”ê°€ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ (í”„ë¡¬í”„íŠ¸ ì¤‘ì•™ ê´€ë¦¬ ì ìš©) ===
    
    def explain_query(self, sql_query: str, question: str) -> dict:
        """
        ìƒì„±ëœ SQL ì¿¼ë¦¬ì— ëŒ€í•œ ì„¤ëª… ìƒì„± (í”„ë¡¬í”„íŠ¸ ì¤‘ì•™ ê´€ë¦¬ ì ìš©)
        """
        try:
            explanation_prompt = prompt_manager.get_prompt(
                category='improvements',
                template_name='explain_query',
                user_question=question,
                sql_query=sql_query,
                fallback_prompt=self._get_fallback_explain_prompt(sql_query, question)
            )

            response = self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=600,
                messages=[{"role": "user", "content": explanation_prompt}]
            )
            
            explanation = response.content[0].text.strip()
            logger.info(f"ğŸ“ ì¿¼ë¦¬ ì„¤ëª… ìƒì„± ì™„ë£Œ (ì¤‘ì•™ ê´€ë¦¬)")
            
            return {
                "success": True,
                "explanation": explanation
            }
            
        except Exception as e:
            logger.error(f"âŒ ì¿¼ë¦¬ ì„¤ëª… ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "explanation": None
            }
    
    def suggest_improvements(self, sql_query: str) -> dict:
        """
        SQL ì¿¼ë¦¬ ê°œì„  ì‚¬í•­ ì œì•ˆ (í”„ë¡¬í”„íŠ¸ ì¤‘ì•™ ê´€ë¦¬ ì ìš©)
        """
        try:
            improvement_prompt = prompt_manager.get_prompt(
                category='improvements',
                template_name='suggest_improvements',
                sql_query=sql_query,
                fallback_prompt=self._get_fallback_improvement_prompt(sql_query)
            )

            response = self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=1500,
                messages=[{"role": "user", "content": improvement_prompt}]
            )
            
            suggestions = response.content[0].text.strip()
            logger.info(f"ğŸ’¡ ì¿¼ë¦¬ ê°œì„  ì œì•ˆ ìƒì„± ì™„ë£Œ (ì¤‘ì•™ ê´€ë¦¬)")
            
            return {
                "success": True,
                "suggestions": suggestions
            }
            
        except Exception as e:
            logger.error(f"âŒ ê°œì„  ì œì•ˆ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "suggestions": None
            }
    
    def generate_sample_questions(self, project_id: str, dataset_ids: list = None) -> dict:
        """
        í”„ë¡œì íŠ¸ì™€ ë°ì´í„°ì…‹ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒ˜í”Œ ì§ˆë¬¸ë“¤ì„ ìƒì„± (í”„ë¡¬í”„íŠ¸ ì¤‘ì•™ ê´€ë¦¬ ì ìš©)
        """
        try:
            dataset_info = ""
            if dataset_ids:
                dataset_list = ", ".join(dataset_ids)
                dataset_info = f"ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹: {dataset_list}"
            
            sample_prompt = prompt_manager.get_prompt(
                category='guides',
                template_name='sample_questions',
                project_id=project_id,
                dataset_info=dataset_info,
                fallback_prompt=self._get_fallback_sample_questions_prompt(project_id, dataset_info)
            )

            response = self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=1000,
                messages=[{"role": "user", "content": sample_prompt}]
            )
            
            response_text = response.content[0].text.strip()
            
            # JSON ë°°ì—´ íŒŒì‹± ì‹œë„
            try:
                questions = json.loads(response_text)
                if isinstance(questions, list):
                    logger.info(f"ğŸ“ ìƒ˜í”Œ ì§ˆë¬¸ {len(questions)}ê°œ ìƒì„± ì™„ë£Œ (ì¤‘ì•™ ê´€ë¦¬)")
                    return {
                        "success": True,
                        "questions": questions
                    }
            except json.JSONDecodeError:
                # íŒŒì‹± ì‹¤íŒ¨ì‹œ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
                questions = self._extract_questions_from_text(response_text)
                return {
                    "success": True,
                    "questions": questions
                }
            
            return {
                "success": False,
                "error": "ì‘ë‹µ í˜•ì‹ì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                "questions": []
            }
            
        except Exception as e:
            logger.error(f"âŒ ìƒ˜í”Œ ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "questions": []
            }
    
    def _extract_questions_from_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì§ˆë¬¸ë“¤ì„ ì¶”ì¶œí•˜ëŠ” í—¬í¼ í•¨ìˆ˜ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)"""
        questions = []
        
        # ë‹¤ì–‘í•œ íŒ¨í„´ìœ¼ë¡œ ì§ˆë¬¸ ì¶”ì¶œ
        patterns = [
            r'"([^"]+\?)"',  # ë”°ì˜´í‘œë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ì§ˆë¬¸
            r'(\d+\.\s+[^?\n]+\?)',  # ë²ˆí˜¸ê°€ ë¶™ì€ ì§ˆë¬¸
            r'([A-Zê°€-í£][^?\n]*\?)',  # ëŒ€ë¬¸ì/í•œê¸€ë¡œ ì‹œì‘í•˜ëŠ” ì§ˆë¬¸
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                clean_question = match.strip()
                if clean_question and len(clean_question) > 10:
                    questions.append(clean_question)
        
        # ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ 10ê°œë¡œ ì œí•œ
        unique_questions = list(dict.fromkeys(questions))[:10]
        
        # ê¸°ë³¸ ì§ˆë¬¸ë“¤ì´ ì—†ì„ ê²½ìš° ê¸°ë³¸ê°’ ì œê³µ
        if not unique_questions:
            unique_questions = [
                "ì „ì²´ ë°ì´í„°ì˜ í–‰ ìˆ˜ëŠ” ì–¼ë§ˆë‚˜ ë˜ë‚˜ìš”?",
                "ê°€ì¥ ìµœê·¼ ë°ì´í„°ëŠ” ì–¸ì œì¸ê°€ìš”?",
                "ê° ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„° ìˆ˜ëŠ”?",
                "ì›”ë³„ ë°ì´í„° ì¶”ì´ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
                "ìƒìœ„ 10ê°œ í•­ëª©ì€ ë¬´ì—‡ì¸ê°€ìš”?"
            ]
        
        return unique_questions

    # === Fallback í”„ë¡¬í”„íŠ¸ë“¤ (í”„ë¡¬í”„íŠ¸ ë¡œë”© ì‹¤íŒ¨ ì‹œ ì‚¬ìš©) ===
    
    def _get_fallback_classification_prompt(self) -> str:
        """ë¶„ë¥˜ í”„ë¡¬í”„íŠ¸ Fallback"""
        return """ì‚¬ìš©ì ì…ë ¥ì„ ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•˜ê³  JSONìœ¼ë¡œ ì‘ë‹µ:
1. query_request - ë°ì´í„° ì¡°íšŒ ìš”ì²­
2. metadata_request - í…Œì´ë¸” ì •ë³´ ìš”ì²­
3. data_analysis - ë°ì´í„° ë¶„ì„ ìš”ì²­
4. guide_request - ì‚¬ìš©ë²• ìš”ì²­
5. out_of_scope - ê¸°ëŠ¥ ë²”ìœ„ ì™¸

JSON í˜•ì‹: {"category": "ë¶„ë¥˜", "confidence": 0.95}"""
    
    def _get_fallback_sql_system_prompt(self, project_id: str, default_table: str) -> str:
        """SQL ìƒì„± ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ Fallback"""
        return f"""BigQuery SQL ì „ë¬¸ê°€ë¡œì„œ ìì—°ì–´ë¥¼ SQLë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.
í”„ë¡œì íŠ¸: {project_id}
ê¸°ë³¸ í…Œì´ë¸”: {default_table}
- SQLë§Œ ë°˜í™˜, ì„¸ë¯¸ì½œë¡  í•„ìˆ˜
- LIMIT 100 ê¸°ë³¸ ì ìš©
- TIMESTAMP_MICROS(event_timestamp) ì‚¬ìš©"""
    
    def _get_fallback_metadata_prompt(self, question: str, table_info: dict, schema_text: str) -> str:
        """ë©”íƒ€ë°ì´í„° ì‘ë‹µ Fallback"""
        return f"""í…Œì´ë¸” ì •ë³´ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”:
{table_info.get('table_id', 'Unknown')}
ì§ˆë¬¸: {question}
ìŠ¤í‚¤ë§ˆ: {schema_text}"""
    
    def _get_fallback_analysis_prompt(self, question: str, data_context: str) -> str:
        """ë°ì´í„° ë¶„ì„ í”„ë¡¬í”„íŠ¸ Fallback"""
        return f"""ë‹¤ìŒ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:
{data_context}
ì§ˆë¬¸: {question}
ì£¼ìš” íŠ¹ì§•ê³¼ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”."""
    
    def _get_fallback_guide_prompt(self, question: str, context: str) -> str:
        """ê°€ì´ë“œ í”„ë¡¬í”„íŠ¸ Fallback"""
        return f"""BigQuery Assistant ì‚¬ìš©ë²•ì„ ì•ˆë‚´í•´ì£¼ì„¸ìš”.
ìƒí™©: {context}
ì§ˆë¬¸: {question}
ì£¼ìš” ê¸°ëŠ¥ê³¼ ì‚¬ìš© ì˜ˆì‹œë¥¼ ì œê³µí•´ì£¼ì„¸ìš”."""
    
    def _get_fallback_out_of_scope_prompt(self, question: str) -> str:
        """ë²”ìœ„ ì™¸ ì‘ë‹µ Fallback"""
        return f"""ì£„ì†¡í•©ë‹ˆë‹¤. '{question}' ì§ˆë¬¸ì€ BigQuery Assistantì˜ ê¸°ëŠ¥ ë²”ìœ„ë¥¼ ë²—ì–´ë‚©ë‹ˆë‹¤.
ëŒ€ì‹  ë°ì´í„° ì¡°íšŒ, ë¶„ì„, í…Œì´ë¸” ì •ë³´ ìš”ì²­ ë“±ì„ ë„ì™€ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."""
    
    def _get_fallback_explain_prompt(self, sql_query: str, question: str) -> str:
        """SQL ì„¤ëª… í”„ë¡¬í”„íŠ¸ Fallback"""
        return f"""ë‹¤ìŒ SQLì„ ì„¤ëª…í•´ì£¼ì„¸ìš”:
ì›ë³¸ ì§ˆë¬¸: {question}
SQL: {sql_query}
ì¿¼ë¦¬ì˜ ëª©ì ê³¼ ê²°ê³¼ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”."""
    
    def _get_fallback_improvement_prompt(self, sql_query: str) -> str:
        """SQL ê°œì„  í”„ë¡¬í”„íŠ¸ Fallback"""
        return f"""ë‹¤ìŒ SQLì˜ ê°œì„  ë°©ì•ˆì„ ì œì•ˆí•´ì£¼ì„¸ìš”:
{sql_query}
ì„±ëŠ¥, ë¹„ìš©, ê°€ë…ì„± ê´€ì ì—ì„œ ê°œì„ ì•ˆì„ ì œì‹œí•´ì£¼ì„¸ìš”."""
    
    def _get_fallback_sample_questions_prompt(self, project_id: str, dataset_info: str) -> str:
        """ìƒ˜í”Œ ì§ˆë¬¸ í”„ë¡¬í”„íŠ¸ Fallback"""
        return f"""í”„ë¡œì íŠ¸ {project_id}ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ìœ ìš©í•œ ì§ˆë¬¸ ì˜ˆì‹œë¥¼ JSON ë°°ì—´ë¡œ ì œê³µí•´ì£¼ì„¸ìš”.
{dataset_info}
ê¸°ë³¸ ì¡°íšŒ, ì§‘ê³„, ë¶„ì„ ë“± ë‹¤ì–‘í•œ ì§ˆë¬¸ì„ í¬í•¨í•´ì£¼ì„¸ìš”."""


class LLMClientFactory:
    """LLM í´ë¼ì´ì–¸íŠ¸ íŒ©í† ë¦¬ - í™•ì¥ ê°€ëŠ¥í•œ êµ¬ì¡° (í”„ë¡¬í”„íŠ¸ ì¤‘ì•™ ê´€ë¦¬ ì§€ì›)"""
    
    @staticmethod
    def create_client(provider: str, config: dict) -> BaseLLMClient:
        """
        LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        
        Args:
            provider: LLM ì œê³µì—…ì²´ ('anthropic', 'openai' ë“±)
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬ (api_key ë“±)
            
        Returns:
            BaseLLMClient ì¸ìŠ¤í„´ìŠ¤ (í”„ë¡¬í”„íŠ¸ ì¤‘ì•™ ê´€ë¦¬ ì§€ì›)
            
        Raises:
            ValueError: ì§€ì›í•˜ì§€ ì•ŠëŠ” í”„ë¡œë°”ì´ë”ì¸ ê²½ìš°
        """
        providers = {
            "anthropic": AnthropicLLMClient,
            # í–¥í›„ ì¶”ê°€ ê°€ëŠ¥: "openai": OpenAILLMClient
        }
        
        if provider not in providers:
            supported = ", ".join(providers.keys())
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM í”„ë¡œë°”ì´ë”: {provider}. ì§€ì› ëª©ë¡: {supported}")
        
        try:
            client_class = providers[provider]
            client = client_class(config["api_key"])
            logger.info(f"âœ… {provider} LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì™„ë£Œ (í”„ë¡¬í”„íŠ¸ ì¤‘ì•™ ê´€ë¦¬)")
            return client
        except Exception as e:
            logger.error(f"âŒ {provider} LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise