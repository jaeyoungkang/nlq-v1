"""
LLM Service
LLM ê´€ë ¨ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ë‹´ë‹¹í•˜ëŠ” ì„œë¹„ìŠ¤
"""

import json
from typing import Dict, Any, List, Optional
from core.llm.interfaces import BaseLLMRepository, LLMRequest
from core.prompts import prompt_manager
from core.prompts.fallbacks import FallbackPrompts
from core.models.context import ContextBlock, context_blocks_to_llm_format
from core.config.llm_config import LLMConfigManager
from utils.logging_utils import get_logger
from utils.metasync_cache_loader import get_metasync_cache_loader
from .models import (
    ClassificationRequest, ClassificationResponse,
    SQLGenerationRequest, SQLGenerationResponse,
    AnalysisRequest, AnalysisResponse,
    GuideRequest, OutOfScopeRequest
)
from .utils import (
    clean_sql_response, 
    format_conversation_context, 
    extract_json_from_response,
    sanitize_error_message
)

logger = get_logger(__name__)


class LLMService:
    """LLM ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì„œë¹„ìŠ¤"""
    
    def __init__(self, repository: BaseLLMRepository, cache_loader=None, config_manager: Optional[LLMConfigManager] = None):
        """
        LLM Service ì´ˆê¸°í™”
        
        Args:
            repository: LLM Repository ì¸ìŠ¤í„´ìŠ¤
            cache_loader: MetaSync ìºì‹œ ë¡œë” (ì„ íƒì )
            config_manager: LLM ì„¤ì • ê´€ë¦¬ì (ì„ íƒì )
        """
        self.repository = repository
        self.cache_loader = cache_loader or get_metasync_cache_loader()
        self.config_manager = config_manager or LLMConfigManager()
        logger.info("âœ… LLMService ì´ˆê¸°í™” ì™„ë£Œ (ì„¤ì • ê´€ë¦¬ì í¬í•¨)")
    
    def classify_input(self, request: ClassificationRequest) -> ClassificationResponse:
        """
        ì‚¬ìš©ì ì…ë ¥ ë¶„ë¥˜
        """
        try:
            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë“œ
            system_prompt = prompt_manager.get_prompt(
                category='classification',
                template_name='system_prompt',
                fallback_prompt=FallbackPrompts.classification()
            )
            
            # ContextBlockì„ LLM í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            context_blocks_formatted = ""
            if request.context_blocks:
                context_blocks_formatted = self._format_context_blocks_for_prompt(request.context_blocks)
            else:
                context_blocks_formatted = "[ì´ì „ ëŒ€í™” ì—†ìŒ]"
            
            user_prompt = prompt_manager.get_prompt(
                category='classification',
                template_name='user_prompt',
                user_input=request.user_input,
                context_blocks=context_blocks_formatted,
                fallback_prompt=f"ë‹¤ìŒ ì…ë ¥ì„ ë¶„ë¥˜í•´ì£¼ì„¸ìš”: {request.user_input}"
            )
            
            # ì„¤ì • ê´€ë¦¬ìì—ì„œ classification ì„¤ì • ê°€ì ¸ì˜¤ê¸°
            config = self.config_manager.get_config('classification')
            
            # LLM ìš”ì²­ ìƒì„±
            llm_request = LLMRequest(
                model=config.model_id,
                system=system_prompt,
                messages=[{"role": "user", "content": user_prompt}],
                max_tokens=config.max_tokens,
                temperature=config.temperature
            )
            
            # LLM í˜¸ì¶œ
            response = self.repository.execute_prompt(llm_request)
            
            # JSON ì‘ë‹µ íŒŒì‹±
            result_data = extract_json_from_response(response.content)
            
            if result_data and isinstance(result_data, dict):
                # ì„¤ì •ì—ì„œ confidence ì„ê³„ê°’ ê°€ì ¸ì˜¤ê¸°
                config_confidence = config.confidence or 0.5
                response_confidence = float(result_data.get('confidence', config_confidence))
                
                return ClassificationResponse(
                    category=result_data.get('category', 'unknown'),
                    confidence=response_confidence,
                    reasoning=result_data.get('reasoning')
                )
            else:
                logger.warning("ë¶„ë¥˜ ì‘ë‹µì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŒ, ê¸°ë³¸ê°’ ì‚¬ìš©")
                config_confidence = config.confidence or 0.5
                return ClassificationResponse(
                    category='query_request',
                    confidence=config_confidence,
                    reasoning="íŒŒì‹± ì‹¤íŒ¨"
                )
                
        except Exception as e:
            logger.error(f"ì…ë ¥ ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜: {sanitize_error_message(str(e))}")
            # ì˜¤ë¥˜ ì‹œ ë‚®ì€ confidence ì‚¬ìš©
            return ClassificationResponse(
                category='query_request',
                confidence=0.1,
                reasoning=f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            )
    
    def generate_sql(self, request: SQLGenerationRequest) -> SQLGenerationResponse:
        """
        SQL ìƒì„±
        """
        try:
            # ContextBlockì„ í”„ë¡¬í”„íŠ¸ìš© í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            context_blocks_formatted = ""
            if request.context_blocks:
                context_blocks_formatted = self._format_context_blocks_for_prompt(request.context_blocks)
            else:
                context_blocks_formatted = "[ì´ì „ ëŒ€í™” ì—†ìŒ]"
            
            # MetaSync ë°ì´í„°ë¡œ í…œí”Œë¦¿ ë³€ìˆ˜ ì¤€ë¹„
            template_vars = self._prepare_sql_template_variables(request, context_blocks_formatted)
            
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ - í…œí”Œë¦¿ì— ë§ëŠ” ë³€ìˆ˜ëª… ì‚¬ìš©
            system_prompt = prompt_manager.get_prompt(
                category='sql_generation',
                template_name='system_prompt',
                table_id=template_vars['table_id'],
                schema_columns=template_vars['schema_columns'], 
                few_shot_examples=template_vars['few_shot_examples'],
                fallback_prompt=FallbackPrompts.sql_system(request.project_id, request.default_table)
            )
            
            # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ - í…œí”Œë¦¿ì— ë§ëŠ” ë³€ìˆ˜ëª… ì‚¬ìš©
            user_prompt = prompt_manager.get_prompt(
                category='sql_generation',
                template_name='user_prompt',
                context_blocks=template_vars['context_blocks'],
                question=template_vars['question'],
                fallback_prompt=f"ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ SQLì„ ìƒì„±í•´ì£¼ì„¸ìš”: {request.user_question}"
            )
            
            # ì„¤ì • ê´€ë¦¬ìì—ì„œ sql_generation ì„¤ì • ê°€ì ¸ì˜¤ê¸°
            config = self.config_manager.get_config('sql_generation')
            
            # LLM ìš”ì²­
            llm_request = LLMRequest(
                model=config.model_id,
                system=system_prompt,
                messages=[{"role": "user", "content": user_prompt}],
                max_tokens=config.max_tokens,
                temperature=config.temperature
            )
            
            response = self.repository.execute_prompt(llm_request)
            
            # SQL ì •ë¦¬
            cleaned_sql = clean_sql_response(response.content)
            
            # ì„¤ì •ì—ì„œ confidence ê°€ì ¸ì˜¤ê¸°
            sql_confidence = config.confidence or 0.8
            
            return SQLGenerationResponse(
                sql_query=cleaned_sql,
                explanation=None,  # í•„ìš”ì‹œ ë³„ë„ ì¶”ì¶œ ë¡œì§ êµ¬í˜„
                confidence=sql_confidence
            )
            
        except Exception as e:
            logger.error(f"SQL ìƒì„± ì¤‘ ì˜¤ë¥˜: {sanitize_error_message(str(e))}")
            raise
    
    def analyze_data(self, request: AnalysisRequest) -> AnalysisResponse:
        """
        ë°ì´í„° ë¶„ì„ - ContextBlockì„ ì™„ì „í•œ ì»¨í…ìŠ¤íŠ¸ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        """
        try:
            # ContextBlockì„ ì™„ì „í•œ ì»¨í…ìŠ¤íŠ¸ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            context_json = self._prepare_analysis_context_json(request.context_blocks)
            
            # í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
            system_prompt = prompt_manager.get_prompt(
                category='data_analysis',
                template_name='system_prompt',
                fallback_prompt="ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
            )
            
            # ContextBlock ì™„ì „í•œ ë‹¨ìœ„ë¡œ ì „ë‹¬ (ì„¤ê³„ ì›ì¹™ ì¤€ìˆ˜)
            user_prompt = prompt_manager.get_prompt(
                category='data_analysis',
                template_name='user_prompt',
                context_json=context_json,  # ë‹¨ì¼ ë³€ìˆ˜ë¡œ í†µí•©
                question=request.user_question,
                fallback_prompt=FallbackPrompts.analysis(request.user_question, context_json)
            )
            
            # ì„¤ì • ê´€ë¦¬ìì—ì„œ data_analysis ì„¤ì • ê°€ì ¸ì˜¤ê¸°
            config = self.config_manager.get_config('data_analysis')
            
            # LLM ìš”ì²­
            llm_request = LLMRequest(
                model=config.model_id,
                system=system_prompt,
                messages=[{"role": "user", "content": user_prompt}],
                max_tokens=config.max_tokens,
                temperature=config.temperature
            )
            
            response = self.repository.execute_prompt(llm_request)
            
            return AnalysisResponse(
                analysis=response.content,
                insights=None,  # í•„ìš”ì‹œ êµ¬ì¡°í™” ë¡œì§ ì¶”ê°€
                recommendations=None
            )
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {sanitize_error_message(str(e))}")
            raise
    
    def generate_guide(self, request: GuideRequest) -> str:
        """
        ê°€ì´ë“œ ìƒì„±
        """
        try:
            user_prompt = prompt_manager.get_prompt(
                category='guides',
                template_name='usage_guide',
                question=request.question,
                context=request.context or "",
                fallback_prompt=FallbackPrompts.guide(request.question, request.context or "")
            )
            
            # ì„¤ì • ê´€ë¦¬ìì—ì„œ guide_generation ì„¤ì • ê°€ì ¸ì˜¤ê¸°
            config = self.config_manager.get_config('guide_generation')
            
            llm_request = LLMRequest(
                model=config.model_id,
                messages=[{"role": "user", "content": user_prompt}],
                max_tokens=config.max_tokens,
                temperature=config.temperature
            )
            
            response = self.repository.execute_prompt(llm_request)
            return response.content
            
        except Exception as e:
            logger.error(f"ê°€ì´ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {sanitize_error_message(str(e))}")
            raise
    
    def generate_out_of_scope(self, request: OutOfScopeRequest) -> str:
        """
        ë²”ìœ„ ì™¸ ì‘ë‹µ ìƒì„±
        """
        try:
            user_prompt = prompt_manager.get_prompt(
                category='guides',
                template_name='out_of_scope',
                question=request.question,
                detected_intent=request.detected_intent or "",
                fallback_prompt=FallbackPrompts.out_of_scope(request.question)
            )
            
            # ì„¤ì • ê´€ë¦¬ìì—ì„œ out_of_scope ì„¤ì • ê°€ì ¸ì˜¤ê¸°
            config = self.config_manager.get_config('out_of_scope')
            
            llm_request = LLMRequest(
                model=config.model_id, 
                messages=[{"role": "user", "content": user_prompt}],
                max_tokens=config.max_tokens,
                temperature=config.temperature
            )
            
            response = self.repository.execute_prompt(llm_request)
            return response.content
            
        except Exception as e:
            logger.error(f"ë²”ìœ„ ì™¸ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {sanitize_error_message(str(e))}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. '{request.question}' ì§ˆë¬¸ì€ í˜„ì¬ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê¸°ëŠ¥ì…ë‹ˆë‹¤."
    
    def _prepare_sql_template_variables(self, request: 'SQLGenerationRequest', context_blocks_formatted: str) -> Dict[str, str]:
        """
        SQL ìƒì„± í…œí”Œë¦¿ì„ ìœ„í•œ ë³€ìˆ˜ ì¤€ë¹„
        """
        try:
            # ê¸°ë³¸ ë³€ìˆ˜
            template_vars = {
                'table_id': request.default_table,
                'context_blocks': context_blocks_formatted,
                'question': request.user_question,
                'schema_columns': '',
                'few_shot_examples': ''
            }
            
            # MetaSync ë°ì´í„° ì¶”ê°€
            if self.cache_loader:
                # ìŠ¤í‚¤ë§ˆ ì»¬ëŸ¼ ì •ë³´
                schema_info = self.cache_loader.get_schema_info()
                if schema_info and 'columns' in schema_info:
                    columns = schema_info['columns']
                    column_lines = []
                    for col in columns:
                        col_name = col.get('name', '')
                        col_type = col.get('type', '')
                        col_desc = col.get('description', '')
                        if col_name:
                            column_lines.append(f"- {col_name} ({col_type}): {col_desc}")
                    template_vars['schema_columns'] = '\n'.join(column_lines)
                
                # Few-shot ì˜ˆì‹œ
                examples = self.cache_loader.get_few_shot_examples()
                if examples:
                    example_lines = []
                    for i, example in enumerate(examples[:3], 1):  # ìµœëŒ€ 3ê°œë§Œ
                        question = example.get('question', '')
                        sql = example.get('sql', '')
                        if question and sql:
                            example_lines.append(f"ì˜ˆì‹œ {i}:")
                            example_lines.append(f"ì§ˆë¬¸: {question}")
                            example_lines.append(f"SQL: {sql}")
                            example_lines.append("")
                    template_vars['few_shot_examples'] = '\n'.join(example_lines)
            
            return template_vars
            
        except Exception as e:
            logger.warning(f"SQL í…œí”Œë¦¿ ë³€ìˆ˜ ì¤€ë¹„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                'table_id': request.default_table,
                'context_blocks': context_blocks_formatted,
                'question': request.user_question,
                'schema_columns': '',
                'few_shot_examples': ''
            }
    
    def _prepare_analysis_context_json(self, context_blocks: List[ContextBlock]) -> str:
        """
        ë°ì´í„° ë¶„ì„ì„ ìœ„í•œ context_json ì¤€ë¹„ - ContextBlock ì„¤ê³„ ì˜ë„ ì™„ì „ ì¤€ìˆ˜
        ContextBlock ëª¨ë¸ì˜ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì ê·¹ í™œìš©
        """
        try:
            # ContextBlock ëª¨ë¸ì˜ ì „ìš© ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ í™œìš©
            from core.models.context import create_analysis_context
            context_data = create_analysis_context(context_blocks)
            
            # List[ContextBlock]ë¥¼ ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
            from core.models.context import context_blocks_to_complete_format
            context_data["context_blocks"] = context_blocks_to_complete_format(context_data["context_blocks"])
            
            # ë¡œê¹…
            row_count = context_data["meta"]["total_row_count"]  
            if row_count > 0:
                logger.info(f"ğŸ“Š ë¶„ì„ìš© ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ: {row_count}ê°œ í–‰")
            
            return json.dumps(context_data, ensure_ascii=False, indent=2)
            
        except Exception as e:
            logger.warning(f"ë¶„ì„ ì»¨í…ìŠ¤íŠ¸ JSON ì¤€ë¹„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return '{"context_blocks": [], "meta": {"total_row_count": 0, "blocks_count": 0}, "limits": {"max_rows": 100}}'
    
    def is_available(self) -> bool:
        """
        ì„œë¹„ìŠ¤ ê°€ìš©ì„± í™•ì¸
        """
        return self.repository.is_available()
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        ëª¨ë¸ ì •ë³´ ì¡°íšŒ
        """
        return self.repository.get_model_info()
    
    def _format_context_blocks_for_prompt(self, context_blocks: List[ContextBlock]) -> str:
        """
        ContextBlock ë¦¬ìŠ¤íŠ¸ë¥¼ í”„ë¡¬í”„íŠ¸ìš© ë¬¸ìì—´ë¡œ ë³€í™˜ (ì™„ì „í•œ ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
        ëŒ€í™” + ì¿¼ë¦¬ + ì‹¤í–‰ê²°ê³¼ ë©”íƒ€ì •ë³´ê¹Œì§€ í¬í•¨í•˜ëŠ” ì™„ì „í•œ ì»¨í…ìŠ¤íŠ¸
        """
        if not context_blocks:
            return "[ì´ì „ ëŒ€í™” ì—†ìŒ]"
        
        # ContextBlock ëª¨ë¸ì˜ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ í™œìš©
        from core.models.context import context_blocks_to_llm_format
        recent_blocks = context_blocks[-5:]  # ìµœê·¼ 5ê°œë§Œ
        llm_messages = context_blocks_to_llm_format(recent_blocks)
        
        formatted_parts = []
        conversation_idx = 1
        
        for msg in llm_messages:
            role = "ì‚¬ìš©ì" if msg["role"] == "user" else "AI"
            base_msg = f"[{conversation_idx}] {role}: {msg['content']}"
            
            # AI ì‘ë‹µì˜ ê²½ìš° ì‹¤í–‰ í†µê³„ ì •ë³´ ì¶”ê°€
            if msg["role"] == "assistant" and "metadata" in msg:
                execution_info = []
                
                # ìƒì„±ëœ ì¿¼ë¦¬ ì •ë³´
                if msg["metadata"].get("generated_query"):
                    execution_info.append(f"SQL: {msg['metadata']['generated_query']}")
                
                # ì‹¤í–‰ ê²°ê³¼ í–‰ ìˆ˜ ì •ë³´  
                if msg.get("query_row_count", 0) > 0:
                    execution_info.append(f"ê²°ê³¼: {msg['query_row_count']}ê°œ í–‰")
                
                if execution_info:
                    base_msg += f" ({', '.join(execution_info)})"
                
                conversation_idx += 1
            
            formatted_parts.append(base_msg)
        
        return "\n".join(formatted_parts) if formatted_parts else "[ì´ì „ ëŒ€í™” ì—†ìŒ]"